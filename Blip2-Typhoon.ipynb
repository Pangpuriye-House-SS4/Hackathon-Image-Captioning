{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets accelerate wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpcnc/cloud/SuperAI/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"scb10x/typhoon-7b\",use_fast=False)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data from COCO MS 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-26 14:24:50--  http://images.cocodataset.org/zips/test2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.47.76, 3.5.27.22, 3.5.25.202, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.47.76|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6646970404 (6.2G) [application/zip]\n",
      "Saving to: ‘/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test2017.zip’\n",
      "\n",
      "test2017.zip        100%[===================>]   6.19G  9.78MB/s    in 12m 43s \n",
      "\n",
      "2024-04-26 14:37:34 (8.31 MB/s) - ‘/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test2017.zip’ saved [6646970404/6646970404]\n",
      "\n",
      "--2024-04-26 14:37:34--  http://images.cocodataset.org/zips/train2017.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.53.65, 52.217.90.20, 52.216.43.25, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.53.65|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 19336861798 (18G) [application/zip]\n",
      "Saving to: ‘/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/train2017.zip’\n",
      "\n",
      "train2017.zip        24%[===>                ]   4.40G  9.83MB/s    eta 25m 10s^C\n"
     ]
    }
   ],
   "source": [
    "!wget -P /home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data  http://images.cocodataset.org/zips/test2017.zip\n",
    "!wget -P /home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data  http://images.cocodataset.org/zips/train2017.zip\n",
    "#. Download file from COCO dataset \n",
    "\n",
    "!unzip -q /home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test2017.zip\n",
    "!unzip -q /home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/train2017.zip\n",
    "#. unzip file from COCO dataset \n",
    "\n",
    "!rm /home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test2017.zip\n",
    "!rm /home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/train2017.zip\n",
    "#. Delete zip file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce data size with reduce by Tokensize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = list(Path(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/train2017/\").glob(\"*.jpg\"))\n",
    "labels = pd.read_csv(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/labelv2.csv\")\n",
    "#. I have Label3 it Actually very good but no clean path\n",
    "images = []\n",
    "\n",
    "for name_file in tqdm(labels['image']):\n",
    "    temp_str = name_file.split(\"/\")\n",
    "    if temp_str[0] == \"train2017\":\n",
    "        images.append(str(images_path[0].parent / (temp_str[-1] + \".jpg\")))\n",
    "        # images.append(Image.open(images_path[0].parent / (temp_str[-1] + \".jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels #+ Display data in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['tokens'] = labels['captions'].apply(\n",
    "    lambda x: len(tokenizer(x)[\"input_ids\"])\n",
    ")\n",
    "labels = labels[labels.tokens < 30] #. Reduce Token by fig Max Lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['image'] #+ Display in column Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = labels.drop_duplicates(subset=\"image\")\n",
    "select_labels = labels[labels['split'] == \"train\" ]['captions']\n",
    "select_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path  = \"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data\"\n",
    "images_filter = []\n",
    "database = list(labels[labels['split'] == \"train\" ]['image'])\n",
    "\n",
    "for i in tqdm(list(labels['image'])):\n",
    "    temp = (i.split(\"/\")[-2] + \"/\" + i.split(\"/\")[-1].split(\".\")[0])\n",
    "    if temp in database:\n",
    "        images_filter.append(training_path+\"/\"+i+\".jpg\")\n",
    "\n",
    "print(\"count data image to train\",len(images_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_dict({\"image\": images_filter, \"text\": select_labels})\n",
    "dataset = dataset.train_test_split(test_size=0.85, seed=42) #! In this Line Adjust Training size from TrainTestSplit\n",
    "dataset\n",
    "\n",
    "#! ไม่ควรทำอย่างมากในความเป็นจริงไม่ควรสุ่ม Data เข้าไป Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#. Class for Dataset \n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        im = Image.open(item['image'])\n",
    "        encoding = self.processor(images=im, padding=\"max_length\", return_tensors=\"pt\")   #? remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding[\"text\"] = item[\"text\"]\n",
    "        return encoding\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # pad the input_ids and attention_mask\n",
    "    processed_batch = {}\n",
    "    for key in batch[0].keys():\n",
    "        if key != \"text\":\n",
    "            processed_batch[key] = torch.stack([example[key] for example in batch])\n",
    "        else:\n",
    "            text_inputs = processor.tokenizer(\n",
    "                [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n",
    "            )\n",
    "            processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
    "            processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
    "            \n",
    "    return processed_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#. Init regular Blip-2 model from Transformer Library and Load pretrain model from Hugging Face\n",
    "#. Model -- Blip2 (Image) with Opt (Text) and Pretrain on COCO dataset\n",
    "#. Config -- Optimize on Float16 for high Speed on GPUs and Load only on 8 Bit data \n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b-coco\",\n",
    "            load_in_8bit=True,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b-coco\")\n",
    "\n",
    "#. Use Typhoon 7b as tokenizer -> 35k tokens in Thai Word\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"scb10x/typhoon-7b\",use_fast=False)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.text_config.vocab_size = len(tokenizer)\n",
    "model.language_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "processor.tokenizer = tokenizer\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "#. Let's define the LoraConfig\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]\n",
    ")\n",
    "\n",
    "#. Parameter effective fine-tuning\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 14\n",
    "train_dataset = ImageCaptioningDataset(dataset['train'], processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "test_dataset = ImageCaptioningDataset(dataset['test'], processor)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Number of Iteration per Epoch = {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"model_blip2(H)_Typhoon\"\n",
    "#. Folder name model\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=6e-4, weight_decay=1e-2)\n",
    "#. Optimizer\n",
    "\n",
    "EPOCH = 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    model.train()\n",
    "    \n",
    "    training_loss = 0\n",
    "    validate_loss = 0\n",
    "    \n",
    "    idx = 0\n",
    "\n",
    "    #! เพิ่มการ Evaluate ระหว่างการ Train ไปด้วยแล้วก็ควร เพิ่ม Validation set\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device, torch.float16)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=input_ids\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(\"Training loss\", training_loss / len(train_dataloader))\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    save_dir = os.path.join(experiment_name, str(epoch))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.save_pretrained(save_dir)  #? save on adpater load from pretrain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Litter bit test\n",
    "im = dataset['test'].select([500,501])['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = processor(im, return_tensors=\"pt\").to(device, torch.float16).pixel_values\n",
    "model.eval()\n",
    "\n",
    "outputs = model.generate(pixel_values=pixel_values)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_caption = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(type(generated_caption),generated_caption[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/model/model_blip2(F)_Typhoon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to test 2 path\n",
    "#? 1. test/food/\n",
    "#? 2. test/travel/\n",
    "#? 3. test2017/\n",
    "\n",
    "import datasets\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "images_test = []\n",
    "\n",
    "image1_path = list(Path(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test2017\").glob(\"*.jpg\"))\n",
    "image2_path = list(Path(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test/food\").glob(\"*.jpg\"))\n",
    "image3_path = list(Path(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test/travel\").glob(\"*.jpg\"))\n",
    "\n",
    "image1_path.extend(image2_path)\n",
    "image1_path.extend(image3_path)\n",
    "\n",
    "merge_path = image1_path\n",
    "data_size = len(merge_path)\n",
    "\n",
    "\n",
    "for i in tqdm(image1_path, desc=\"Load image to List\"):\n",
    "    images_test.append(Image.open(i))\n",
    "\n",
    "data_testset = datasets.Dataset.from_dict({\"image\": images_test })\n",
    "\n",
    "print(f\"row in submission : {data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_load = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b-coco\",\n",
    "            load_in_8bit=True,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "#. Load Tokenizer and Processor\n",
    "processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b-coco\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"scb10x/typhoon-7b\",use_fast=False)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model_load.config.text_config.vocab_size = len(tokenizer)\n",
    "model_load.language_model.resize_token_embeddings(len(tokenizer))\n",
    "processor.tokenizer = tokenizer\n",
    "\n",
    "#. Set ID Token\n",
    "model_load.config.eos_token_id = 6\n",
    "\n",
    "#. Adapter path\n",
    "model_load.load_adapter(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/model_blip2(c)_Typhoon/8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "from PIL import Image\n",
    "#? For Test Only  is not submission 40 Test\n",
    "for i in tqdm(range(3,4), desc=\"Genarated Answer Vector : \"):\n",
    "    im = data_testset.select([i])['image'][0]\n",
    "    \n",
    "    pixel_values = processor(im, return_tensors=\"pt\").to(device, torch.float16).pixel_values\n",
    "    model_load.eval()\n",
    "    outputs.append(model_load.generate(pixel_values=pixel_values,\n",
    "                            num_beams=5,\n",
    "                            no_repeat_ngram_size=2,))\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test2017/000000360936.jpg\")\n",
    "\n",
    "pixel_values = processor(im, return_tensors=\"pt\").to(device, torch.float16).pixel_values\n",
    "model_load.eval()\n",
    "outputs = model_load.generate(pixel_values=pixel_values,\n",
    "                              num_beams=5,\n",
    "                              no_repeat_ngram_size=4,)\n",
    "im\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_caption = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/resource/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = submission.image_id.tolist()\n",
    "test_images = [\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/\"+im + \".jpg\" for im in test_images]\n",
    "\n",
    "\n",
    "batch = 58\n",
    "test_prediction = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_images), batch) ,desc=\"Load submission : \"):\n",
    "    images = [Image.open(im) for im in test_images[i: i+batch]]\n",
    "    pixel_values = processor(images, return_tensors=\"pt\").to(\"cuda\", torch.float16).pixel_values\n",
    "\n",
    "    outputs = model_load.generate(pixel_values=pixel_values,\n",
    "                                 num_beams=5,\n",
    "                                 no_repeat_ngram_size=4,)\n",
    "                                  \n",
    "    generated_caption = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    test_prediction.extend(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['caption'].iloc[3:] = test_prediction[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DONTWANT = \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ,.!@#$%^&*()_+[]:;\\'\\\"?\"\n",
    "for i in tqdm(range(48673)):\n",
    "    for j in submission['caption'][i]:\n",
    "        if j in list(DONTWANT):\n",
    "            submission['caption'][i] = \"ไม่ทราบ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/resource/gg6.csv', index=False,encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['image_id'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/resource/gg6.csv\")\n",
    "word_in_caption ,size = [] , len(df2)\n",
    "\n",
    "for i in tqdm(range(size)):\n",
    "    \n",
    "    if str(df2['image_id'][i])[0:4] != \"test\":\n",
    "        df2 = df2.drop(i)\n",
    "\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2.to_csv('/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/resource/gg7.csv', index=False,encoding=\"utf-8\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
