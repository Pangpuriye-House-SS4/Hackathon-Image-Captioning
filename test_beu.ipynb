{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkYE44Iw8C6x"
      },
      "source": [
        "# BLEU Score sacrebleu with references_per_prediction = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVp5AoyY7jw_",
        "metadata": {},
        "outputId": "ac21120e-a653-4432-d144-3b611f20e090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "     ---------------------------------------- 0.0/58.0 kB ? eta -:--:--\n",
            "     ------------- ------------------------ 20.5/58.0 kB 320.0 kB/s eta 0:00:01\n",
            "     -------------------------------------- 58.0/58.0 kB 758.7 kB/s eta 0:00:00\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in c:\\users\\toon\\anaconda3\\envs\\.ai_conda\\lib\\site-packages (from sacrebleu) (2024.4.16)\n",
            "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
            "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\toon\\anaconda3\\envs\\.ai_conda\\lib\\site-packages (from sacrebleu) (1.26.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\toon\\anaconda3\\envs\\.ai_conda\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
            "Collecting lxml (from sacrebleu)\n",
            "  Downloading lxml-5.2.1-cp312-cp312-win_amd64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: pywin32>=226 in c:\\users\\toon\\anaconda3\\envs\\.ai_conda\\lib\\site-packages (from portalocker->sacrebleu) (306)\n",
            "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "   ---------------------------------------- 0.0/106.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 106.7/106.7 kB 6.0 MB/s eta 0:00:00\n",
            "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Downloading lxml-5.2.1-cp312-cp312-win_amd64.whl (3.8 MB)\n",
            "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.2/3.8 MB 7.3 MB/s eta 0:00:01\n",
            "   ------ --------------------------------- 0.6/3.8 MB 8.0 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 1.0/3.8 MB 8.3 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 1.4/3.8 MB 8.2 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 1.8/3.8 MB 8.6 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 2.1/3.8 MB 7.8 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 2.3/3.8 MB 7.4 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 2.4/3.8 MB 7.1 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 2.8/3.8 MB 6.8 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 3.1/3.8 MB 6.8 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 3.2/3.8 MB 7.0 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 3.2/3.8 MB 6.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 3.3/3.8 MB 5.8 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 3.6/3.8 MB 5.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.8/3.8 MB 5.7 MB/s eta 0:00:00\n",
            "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: tabulate, portalocker, lxml, sacrebleu\n",
            "Successfully installed lxml-5.2.1 portalocker-2.8.2 sacrebleu-2.4.2 tabulate-0.9.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sNvnFcj9JHE"
      },
      "source": [
        "<li>references_per_prediction = 3 ; เป็น พารามิเตอร์ที่บอกว่าเราจะใช้ 3 reference translation for each prediction translation</li>\n",
        "<li>ไอตัวนี้มันมันใช้ในการเปรียบเทียบ <code>machine translations with human translations ให้ prop กับเรามา</li>\n",
        "<li>The score [0,1]\n",
        "<ul>\n",
        "    <li>1 is perfect</li>\n",
        "</ul></li>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcxU_jf8_zMY"
      },
      "source": [
        "<h3>Ex1</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0gg0DGyA8KI"
      },
      "source": [
        "<li>ตัวเลขตัวนี้ที่ได้คำตอบมา คำเป็นการเปรียบเทียบคุณภาพ กับ ประโยคที่ได้ เทียบกับ reference sentence</li>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hSPmwMRU7WNg",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from sacrebleu import corpus_bleu, sentence_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lCdatuaR_n8i",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Corresponding reference translations\n",
        "references = [\n",
        "    [\"This is a test translation.\"],\n",
        "    [\"Here's another example.\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JAJn7NDn7aHt",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Predicted translation from a model\n",
        "predictions = \"This is a trial translation.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnmK3kTM7aCP",
        "metadata": {},
        "outputId": "7c2b366e-3514-467f-ce04-510fb3be9144"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "BLEU: Each element of `refs` should be a string.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43msentence_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu_score\u001b[38;5;241m.\u001b[39mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Toon\\anaconda3\\envs\\.ai_conda\\Lib\\site-packages\\sacrebleu\\compat.py:91\u001b[0m, in \u001b[0;36msentence_bleu\u001b[1;34m(hypothesis, references, smooth_method, smooth_value, lowercase, tokenize, use_effective_order)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03mComputes BLEU for a single sentence against a single (or multiple) reference(s).\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m:return: Returns a `BLEUScore` object.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m metric \u001b[38;5;241m=\u001b[39m BLEU(\n\u001b[0;32m     87\u001b[0m     lowercase\u001b[38;5;241m=\u001b[39mlowercase, tokenize\u001b[38;5;241m=\u001b[39mtokenize, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m     smooth_method\u001b[38;5;241m=\u001b[39msmooth_method, smooth_value\u001b[38;5;241m=\u001b[39msmooth_value,\n\u001b[0;32m     89\u001b[0m     effective_order\u001b[38;5;241m=\u001b[39muse_effective_order)\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Toon\\anaconda3\\envs\\.ai_conda\\Lib\\site-packages\\sacrebleu\\metrics\\bleu.py:420\u001b[0m, in \u001b[0;36mBLEU.sentence_score\u001b[1;34m(self, hypothesis, references)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_order:\n\u001b[0;32m    418\u001b[0m     sacrelogger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIt is recommended to enable `effective_order` for sentence-level BLEU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Toon\\anaconda3\\envs\\.ai_conda\\Lib\\site-packages\\sacrebleu\\metrics\\base.py:395\u001b[0m, in \u001b[0;36mMetric.sentence_score\u001b[1;34m(self, hypothesis, references)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, hypothesis: \u001b[38;5;28mstr\u001b[39m, references: Sequence[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the metric for a single sentence against a single (or multiple) reference(s).\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m    :param hypothesis: A single hypothesis string.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    :param references: A sequence of reference strings.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    :return: A `Score` object.\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_sentence_score_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_corpus_statistics(\n\u001b[0;32m    398\u001b[0m         [hypothesis], [[refs] \u001b[38;5;28;01mfor\u001b[39;00m refs \u001b[38;5;129;01min\u001b[39;00m references])\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_and_compute(stats)\n",
            "File \u001b[1;32mc:\\Users\\Toon\\anaconda3\\envs\\.ai_conda\\Lib\\site-packages\\sacrebleu\\metrics\\base.py:227\u001b[0m, in \u001b[0;36mMetric._check_sentence_score_args\u001b[1;34m(self, hyp, refs)\u001b[0m\n\u001b[0;32m    224\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEach element of `refs` should be a string.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_msg:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: BLEU: Each element of `refs` should be a string."
          ]
        }
      ],
      "source": [
        "# Calculate BLEU score\n",
        "bleu_score = sentence_bleu(predictions, references)\n",
        "print(f\"BLEU score: {bleu_score.score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbybiQM6_1gN"
      },
      "source": [
        "<h2>Ex2</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lraI_JzC7aAd",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Predicted translations from a model\n",
        "predictions = [\"This is a test translation.\",\n",
        "               \"Here is another sample.\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "awtX_5Uh_8CP",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# Corresponding reference translations\n",
        "references = [\n",
        "    [\"This is a test translation.\", \"This is a trial translation.\", \"This is an example translation.\"],\n",
        "    [\"Here's another example.\", \"This is another example.\", \"Here is another sample.\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6riXRzY_7_N",
        "metadata": {},
        "outputId": "9a91a5d0-0913-4296-a6ed-be8cb1c89988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score: 65.76092928889221\n"
          ]
        }
      ],
      "source": [
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu(predictions, references)\n",
        "print(f\"BLEU score: {bleu_score.score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o97fNxrhAA6H"
      },
      "source": [
        "<h2>Ex3</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPZhplbC_78l",
        "metadata": {},
        "outputId": "8a78543c-4b8e-4985-a346-431fc6b3b316"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score: 100.00000000000004\n"
          ]
        }
      ],
      "source": [
        "# Predicted translations from a model\n",
        "predictions = [\"This is a test translation\",\n",
        "               \"This is another example.\"]\n",
        "# Corresponding reference translations\n",
        "references = [\n",
        "    [\"This is a test translation.\", \"This is a trial translation.\", \"This is an example translation.\"],\n",
        "    [\"Here's another example.\", \"This is another example.\", \"Here is another sample.\"]\n",
        "]\n",
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu(predictions, references)\n",
        "print(f\"BLEU score: {bleu_score.score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBaYC6uIBHCh",
        "metadata": {},
        "outputId": "90344ed7-648b-46c9-dc90-9e474121cfeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score: 22.957488466614336\n"
          ]
        }
      ],
      "source": [
        "# Predicted translations from a model\n",
        "predictions = [\"This is an example translation.\"]\n",
        "# Corresponding reference translations\n",
        "references = [\n",
        "    [\"This is a test translation.\", \"This is a trial translation.\", \"This is an example translation.\"],\n",
        "]\n",
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu(predictions, references)\n",
        "print(f\"BLEU score: {bleu_score.score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thdMsXeFCbLT",
        "metadata": {},
        "outputId": "4cf28c1f-b0ad-4fd1-9871-36ff4728a6c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score: 100.00000000000004\n"
          ]
        }
      ],
      "source": [
        "# Predicted translations from a model\n",
        "predictions = [\"This is a test translation.\"]\n",
        "# Corresponding reference translations\n",
        "references = [\n",
        "    [\"This is a test translation.\", \"This is a trial translation.\", \"This is an example translation.\"],\n",
        "]\n",
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu(predictions, references)\n",
        "print(f\"BLEU score: {bleu_score.score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDnQHhu_DT8x"
      },
      "source": [
        "<li>สิ่งที่มันใช้วัดนั้นคือ N-gram\n",
        "<li>บางประโยค 1-gram อาจจะได้ high score แต่ว่า พอ longer-gram เนี้ยคะแนนคือตก ฉะนั้นแล้วมันก็เลยได้คะแนนน้อย</li>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu2t8qoEFjeC"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfZyJpFROC4X"
      },
      "source": [
        "<h3>ตัดคำด้วย sentencepiece โดยใช้โมเดล flores101</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r28p_XxDMVno"
      },
      "source": [
        "<a href=\"https://www.kaggle.com/competitions/thai-language-image-captioning\">Download link from kaggle flores101</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LFC0HgcDaXc",
        "metadata": {},
        "outputId": "9214320e-72bd-4373-8255-7ba4bc305529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl.metadata (8.3 kB)\n",
            "Downloading sentencepiece-0.2.0-cp312-cp312-win_amd64.whl (991 kB)\n",
            "   ---------------------------------------- 0.0/992.0 kB ? eta -:--:--\n",
            "   ---------------------------------------- 10.2/992.0 kB ? eta -:--:--\n",
            "   - ------------------------------------- 30.7/992.0 kB 660.6 kB/s eta 0:00:02\n",
            "   --- ----------------------------------- 81.9/992.0 kB 770.8 kB/s eta 0:00:02\n",
            "   ------ --------------------------------- 163.8/992.0 kB 1.2 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 286.7/992.0 kB 1.6 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 450.6/992.0 kB 2.0 MB/s eta 0:00:01\n",
            "   ----------------------- ---------------- 583.7/992.0 kB 2.2 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 747.5/992.0 kB 2.4 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 860.2/992.0 kB 2.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 992.0/992.0 kB 2.4 MB/s eta 0:00:00\n",
            "Installing collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwZa09nYGDQ9",
        "metadata": {},
        "outputId": "db0c55d4-216c-4eaa-9a65-075fadc5232c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁Your', '▁text', '▁here']\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('sacrebleu_tokenizer_spm.model')  # Replace with the actual path to the model\n",
        "\n",
        "text = \"Your text here\"\n",
        "tokens = sp.encode_as_pieces(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "metadata": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BLEU = 34.57 50.0/42.9/33.3/20.0 (BP = 1.000 ratio = 1.000 hyp_len = 8 ref_len = 8)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence_bleu('เด็กนั่งเล่นกับเด็กนั่งเล่นกับ', ['เด็กผู้หญิง ในสวนหย่อม', 'เด็กนั่งเล่นกับสุนัข', 'ผู้หญิงสวมเสื้อ เด็กนั่งเล่นกับสุนัข'], tokenize=\"flores101\") # 100.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QU0_YGiGFDB",
        "metadata": {},
        "outputId": "8d1b8bd2-bfb7-4b62-b1c2-252a9b16c29f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁ผู้หญิง', 'สวม', 'เสื้อ', 'แขน', 'ยาว', 'สีขาว', 'และ', 'เด็ก', 'นั่ง', 'เล่น', 'กับ', 'สุนัข', 'อยู่', '▁ใน', 'สวน', 'หย', '่อม']\n"
          ]
        }
      ],
      "source": [
        "text = \"ผู้หญิงสวมเสื้อแขนยาวสีขาวและเด็กนั่งเล่นกับสุนัขอยู่ ในสวนหย่อม\"\n",
        "tokens = sp.encode_as_pieces(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgDs1-asMrkc"
      },
      "source": [
        "<li>Download JSON file</li>\n",
        "<pre>\n",
        "import json\n",
        "# Load the JSON file\n",
        "with open(r'C:\\Users\\Acer\\Desktop\\Hackathon1\\ipu24_v0.4.1_coco.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "for i,j in enumerate(data.items()):\n",
        "    if i == 10:\n",
        "        break\n",
        "    for a in j[1]:\n",
        "        print(a)\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cqVNh3_eGE_l",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "text_example = [\n",
        "    \"ผู้หญิงสวมเสื้อแขนยาวสีขาวและเด็กนั่งเล่นกับสุนัขอยู่ ในสวนหย่อม\",\n",
        "    \"สาวคนนึงกำลังพาเด็กมานั่งเล่นอยู่ภายในสนามหญ้าพร้อมกับสุนัข\",\n",
        "    \"ภาพขาวดำ ผู้หญิงนั่งบนพื้นอุ้มเด็กบนตัก ข้าง ๆ มีหมาสองตัว ด้านหลังมีม้านั่งไม้\",\n",
        "    \"สีน้ำตาลตัวเล็กกำลังกินอาหารอยู่บนจานกระดาษสีขาวข้างแก้วน้ำ\",\n",
        "    \"นกน้อยตัวหนึ่งกำลังจิกกินเศษอาหารที่วางทิ้งไว้บนโต๊ะ\",\n",
        "    \"บนโต๊ะอาหาร นกน้อยตัวหนึ่งกำลังก้มหน้ากินอะไรบางอย่าง\",\n",
        "    \"ไปสัญญาณจราจร 3 อันติดอยู่บนเสาไฟสีดำแสดงสัญญาณไฟสีแดง\",\n",
        "    \"สัญญาณไฟจราจรที่ถูกติดตั้งอยู่กับเสาไฟอยู่ริมถนน\",\n",
        "    \"สัญญาณไฟจราจรที่ได้ติดอยู่บนเสาต้นหนึ่งที่ได้ตั้งอยู่ในริมถนน\",\n",
        "    \"จานสีขาวจานนึงขบวนการนั้นมี โดนัทสอดไส้ทูน่าอยู่ในจาน\",\n",
        "    \"เบอร์เกอร์มีไส้เนื้อสัตว์และซอสสีขาวหั่นครึ่งชิ้นใส่อยู่ในจานทรงกลมสีขาวบนโต๊ะ\",\n",
        "    \"ขนมปังที่เป็นโดนัตวางอยู่บนจานสีขาวที่อยู่บนโต๊ะไม้\",\n",
        "    \"ห้องครัวห้องนึงที่มีตู้เก็บของเป็นสีน้ำตาลวางอยู่หลายตู้และมีอ่างล้างหน้าสีขาว\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVaWeI4QGE8V",
        "metadata": {},
        "outputId": "c89d6843-2ca2-4634-9f11-807792b09b81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁ผู้หญิง', 'สวม', 'เสื้อ', 'แขน', 'ยาว', 'สีขาว', 'และ', 'เด็ก', 'นั่ง', 'เล่น', 'กับ', 'สุนัข', 'อยู่', '▁ใน', 'สวน', 'หย', '่อม']\n",
            "['▁สาว', 'คน', 'นึง', 'กําลัง', 'พา', 'เด็ก', 'มาน', 'ั่ง', 'เล่น', 'อยู่', 'ภายใน', 'สนาม', 'หญ้า', 'พร้อมกับ', 'สุนัข']\n",
            "['▁ภาพ', 'ขาว', 'ดํา', '▁ผู้หญิง', 'นั่ง', 'บน', 'พื้น', 'อุ', '้ม', 'เด็ก', 'บน', 'ตัก', '▁ข', '้าง', '▁ๆ', '▁มี', 'หมา', 'สอง', 'ตัว', '▁ด้าน', 'หลัง', 'มี', 'ม', '้าน', 'ั่ง', 'ไม้']\n",
            "['▁สี', 'น้ําตาล', 'ตัว', 'เล็ก', 'กําลัง', 'กิน', 'อาหาร', 'อยู่บน', 'จาน', 'กระดาษ', 'สีขาว', 'ข้าง', 'แก้ว', 'น้ํา']\n",
            "['▁น', 'ก', 'น้อย', 'ตัว', 'หนึ่ง', 'กําลัง', 'จ', 'ิก', 'กิน', 'เศษ', 'อาหาร', 'ที่', 'วาง', 'ทิ้ง', 'ไว้', 'บน', 'โต๊ะ']\n",
            "['▁บน', 'โต๊ะ', 'อาหาร', '▁น', 'ก', 'น้อย', 'ตัว', 'หนึ่ง', 'กําลัง', 'ก', '้ม', 'หน้า', 'กิน', 'อะไร', 'บางอย่าง']\n",
            "['▁ไป', 'สัญญาณ', 'จ', 'รา', 'จร', '▁3', '▁อัน', 'ติด', 'อยู่บน', 'เสา', 'ไฟ', 'สีดํา', 'แสดง', 'สัญญาณ', 'ไฟ', 'สีแดง']\n",
            "['▁สัญ', 'ญาณ', 'ไฟ', 'จ', 'รา', 'จร', 'ที่ถูก', 'ติดตั้ง', 'อยู่กับ', 'เสา', 'ไฟ', 'อยู่', 'ริม', 'ถนน']\n",
            "['▁สัญ', 'ญาณ', 'ไฟ', 'จ', 'รา', 'จร', 'ที่ได้', 'ติด', 'อยู่บน', 'เสา', 'ต้น', 'หนึ่ง', 'ที่ได้', 'ตั้ง', 'อยู่ใน', 'ริม', 'ถนน']\n",
            "['▁จ', 'าน', 'สีขาว', 'จาน', 'นึง', 'ข', 'บวนการ', 'นั้น', 'มี', '▁โดน', 'ัท', 'สอด', 'ไส้', 'ท', 'ูน', '่า', 'อยู่ใน', 'จาน']\n",
            "['▁เบอร์', 'เกอร์', 'มี', 'ไส้', 'เนื้อ', 'สัตว์', 'และ', 'ซ', 'อส', 'สีขาว', 'ห', 'ั่น', 'ครึ่ง', 'ชิ้น', 'ใส', '่อย', 'ู่', 'ใน', 'จาน', 'ทรง', 'กลม', 'สีขาว', 'บน', 'โต๊ะ']\n",
            "['▁ข', 'นม', 'ปัง', 'ที่เป็น', 'โดน', 'ัต', 'วาง', 'อยู่บน', 'จาน', 'สีขาว', 'ที่อยู่', 'บน', 'โต๊ะ', 'ไม้']\n",
            "['▁ห้อง', 'ครัว', 'ห้อง', 'นึง', 'ที่มี', 'ตู้', 'เก็บ', 'ของ', 'เป็น', 'สี', 'น้ําตาล', 'วาง', 'อยู่', 'หลาย', 'ตู้', 'และมี', 'อ่าง', 'ล้าง', 'หน้า', 'สีขาว']\n"
          ]
        }
      ],
      "source": [
        "for text in text_example:\n",
        "    print(sp.encode_as_pieces(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCLF44DyOQhq"
      },
      "source": [
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkRWndrkOW2y"
      },
      "source": [
        "<h3>วัดคะแนนด้วย BLEU Score แบบ Sacrebleu โดยใช้ references_per_prediction = 3\n",
        "ตัดคำด้วย sentencepiece โดยใช้โมเดล flores101</h3>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIHGzXB3O-vY"
      },
      "source": [
        "<pre>\n",
        "# Load the JSON file\n",
        "with open(r'C:\\Users\\Acer\\Desktop\\Hackathon1\\ipu24_v0.4.1_coco.json', 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "MsToken = []\n",
        "for i,j in enumerate(data.items()):\n",
        "    if i == 10:\n",
        "        break\n",
        "    # print(j[1])\n",
        "    MsToken.append(j[1])\n",
        "MsToken\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nAZITjUPGE5d",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "MsToken = [\n",
        "    ['ผู้หญิงสวมเสื้อแขนยาวสีขาวและเด็กนั่งเล่นกับสุนัขอยู่ ในสวนหย่อม',\n",
        "  'สาวคนนึงกำลังพาเด็กมานั่งเล่นอยู่ภายในสนามหญ้าพร้อมกับสุนัข',\n",
        "  'ภาพขาวดำ ผู้หญิงนั่งบนพื้นอุ้มเด็กบนตัก ข้าง ๆ มีหมาสองตัว ด้านหลังมีม้านั่งไม้'],\n",
        " ['สีน้ำตาลตัวเล็กกำลังกินอาหารอยู่บนจานกระดาษสีขาวข้างแก้วน้ำ',\n",
        "  'นกน้อยตัวหนึ่งกำลังจิกกินเศษอาหารที่วางทิ้งไว้บนโต๊ะ',\n",
        "  'บนโต๊ะอาหาร นกน้อยตัวหนึ่งกำลังก้มหน้ากินอะไรบางอย่าง']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tO4bQpCNRjJI",
        "metadata": {},
        "outputId": "af40c375-9fd2-4984-cc35-8e039d0aa69d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['▁ผู้หญิง สวม เสื้อ แขน ยาว สีขาว และ เด็ก นั่ง เล่น กับ สุนัข อยู่ ▁ใน สวน หย ่อม',\n",
              "  '▁สาว คน นึง กําลัง พา เด็ก มาน ั่ง เล่น อยู่ ภายใน สนาม หญ้า พร้อมกับ สุนัข',\n",
              "  '▁ภาพ ขาว ดํา ▁ผู้หญิง นั่ง บน พื้น อุ ้ม เด็ก บน ตัก ▁ข ้าง ▁ๆ ▁มี หมา สอง ตัว ▁ด้าน หลัง มี ม ้าน ั่ง ไม้'],\n",
              " ['▁สี น้ําตาล ตัว เล็ก กําลัง กิน อาหาร อยู่บน จาน กระดาษ สีขาว ข้าง แก้ว น้ํา',\n",
              "  '▁น ก น้อย ตัว หนึ่ง กําลัง จ ิก กิน เศษ อาหาร ที่ วาง ทิ้ง ไว้ บน โต๊ะ',\n",
              "  '▁บน โต๊ะ อาหาร ▁น ก น้อย ตัว หนึ่ง กําลัง ก ้ม หน้า กิน อะไร บางอย่าง']]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = []\n",
        "for Mt in MsToken:\n",
        "    x = []\n",
        "    for M in Mt:\n",
        "        Ms = sp.encode_as_pieces(M)\n",
        "        Ms = ' '.join(Ms)\n",
        "        x.append(Ms)\n",
        "    y.append(x)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-4NpqbFmTRBe",
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "# # Predicted translations from a model\n",
        "# predictions = [\"This is a test translation\",\n",
        "#                \"This is another example.\"]\n",
        "# # Corresponding reference translations\n",
        "# references = [\n",
        "#     [\"This is a test translation.\", \"This is a trial translation.\", \"This is an example translation.\"],\n",
        "#     [\"Here's another example.\", \"This is another example.\", \"Here is another sample.\"]\n",
        "# ]\n",
        "# # Calculate BLEU score\n",
        "# bleu_score = corpus_bleu(predictions, references)\n",
        "# print(f\"BLEU score: {bleu_score.score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BB_8rGVOTXN8",
        "metadata": {},
        "outputId": "60bf914d-79bf-4186-94d2-03cdf95af751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁ผู้หญิง สวม เสื้อ แขน ยาว สีขาว และ เด็ก นั่ง เล่น กับ สุนัข อยู่ ▁ใน สวน หย ่อม', '▁สี น้ําตาล ตัว เล็ก กําลัง กิน อาหาร อยู่บน จาน กระดาษ สีขาว ข้าง แก้ว น้ํา']\n"
          ]
        }
      ],
      "source": [
        "# Predicted translations from a model\n",
        "predictions = ['ผู้หญิงสวมเสื้อแขนยาวสีขาวและเด็กนั่งเล่นกับสุนัขอยู่ ในสวนหย่อม',\n",
        "               'สีน้ำตาลตัวเล็กกำลังกินอาหารอยู่บนจานกระดาษสีขาวข้างแก้วน้ำ']\n",
        "pd = []\n",
        "for i in predictions:\n",
        "    Mt = sp.encode_as_pieces(i)\n",
        "    Mt = ' '.join(Mt)\n",
        "    pd.append(Mt)\n",
        "print(pd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUo15T_PTjkA",
        "metadata": {},
        "outputId": "c8948edd-fe3e-4090-cb66-1ab964c85452"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "BLEU: The argument `hyp` should be a string.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m references \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Calculate BLEU score\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43msentence_bleu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLEU score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbleu_score\u001b[38;5;241m.\u001b[39mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\Toon\\anaconda3\\envs\\.ai_conda\\Lib\\site-packages\\sacrebleu\\compat.py:91\u001b[0m, in \u001b[0;36msentence_bleu\u001b[1;34m(hypothesis, references, smooth_method, smooth_value, lowercase, tokenize, use_effective_order)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03mComputes BLEU for a single sentence against a single (or multiple) reference(s).\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m:return: Returns a `BLEUScore` object.\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m metric \u001b[38;5;241m=\u001b[39m BLEU(\n\u001b[0;32m     87\u001b[0m     lowercase\u001b[38;5;241m=\u001b[39mlowercase, tokenize\u001b[38;5;241m=\u001b[39mtokenize, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     88\u001b[0m     smooth_method\u001b[38;5;241m=\u001b[39msmooth_method, smooth_value\u001b[38;5;241m=\u001b[39msmooth_value,\n\u001b[0;32m     89\u001b[0m     effective_order\u001b[38;5;241m=\u001b[39muse_effective_order)\n\u001b[1;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Toon\\anaconda3\\envs\\.ai_conda\\Lib\\site-packages\\sacrebleu\\metrics\\bleu.py:420\u001b[0m, in \u001b[0;36mBLEU.sentence_score\u001b[1;34m(self, hypothesis, references)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_order:\n\u001b[0;32m    418\u001b[0m     sacrelogger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIt is recommended to enable `effective_order` for sentence-level BLEU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Toon\\anaconda3\\envs\\.ai_conda\\Lib\\site-packages\\sacrebleu\\metrics\\base.py:395\u001b[0m, in \u001b[0;36mMetric.sentence_score\u001b[1;34m(self, hypothesis, references)\u001b[0m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, hypothesis: \u001b[38;5;28mstr\u001b[39m, references: Sequence[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the metric for a single sentence against a single (or multiple) reference(s).\u001b[39;00m\n\u001b[0;32m    390\u001b[0m \n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m    :param hypothesis: A single hypothesis string.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    :param references: A sequence of reference strings.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    :return: A `Score` object.\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_sentence_score_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhypothesis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    397\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_corpus_statistics(\n\u001b[0;32m    398\u001b[0m         [hypothesis], [[refs] \u001b[38;5;28;01mfor\u001b[39;00m refs \u001b[38;5;129;01min\u001b[39;00m references])\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_and_compute(stats)\n",
            "File \u001b[1;32mc:\\Users\\Toon\\anaconda3\\envs\\.ai_conda\\Lib\\site-packages\\sacrebleu\\metrics\\base.py:227\u001b[0m, in \u001b[0;36mMetric._check_sentence_score_args\u001b[1;34m(self, hyp, refs)\u001b[0m\n\u001b[0;32m    224\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEach element of `refs` should be a string.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_msg:\n\u001b[1;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: BLEU: The argument `hyp` should be a string."
          ]
        }
      ],
      "source": [
        "predictions = pd[0]\n",
        "# Corresponding reference translations\n",
        "references = y\n",
        "# Calculate BLEU score\n",
        "bleu_score = sentence_bleu(predictions, references)\n",
        "print(f\"BLEU score: {bleu_score.score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
