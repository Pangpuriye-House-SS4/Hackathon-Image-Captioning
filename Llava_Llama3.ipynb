{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets accelerate wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library and Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpcnc/cloud/SuperAI/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-24 10:50:07.743310: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-24 10:50:08.327552: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360216/360216 [00:01<00:00, 358107.21it/s]\n"
     ]
    }
   ],
   "source": [
    "images_path = list(Path(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/train2017/\").glob(\"*.jpg\"))\n",
    "labels = pd.read_csv(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/label-nocleaning.csv\")\n",
    "\n",
    "images = []\n",
    "\n",
    "for name_file in tqdm(labels['image']):\n",
    "    temp_str = name_file.split(\"/\")\n",
    "    if temp_str[0] == \"train2017\":\n",
    "        images.append(str(images_path[0].parent / (temp_str[-1] + \".jpg\")))\n",
    "        # Image.open(str(images_path[0].parent) / temp_str[-1]+\".jpg\")\n",
    "\n",
    "# images = [Image.open(str(images_path[0].parent / (path.split(\"/\")[-1]+\".jpg\") )) for path in labels['image']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>captions</th>\n",
       "      <th>split_id</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train2017/000000373716</td>\n",
       "      <td>ผู้หญิงสวมเสื้อแขนยาวสีขาวและเด็กนั่งเล่นกับสุ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train2017/000000373716</td>\n",
       "      <td>สาวคนนึงกำลังพาเด็กมานั่งเล่นอยู่ภายในสนามหญ้า...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train2017/000000373716</td>\n",
       "      <td>ภาพขาวดำ ผู้หญิงนั่งบนพื้นอุ้มเด็กบนตัก ข้าง ๆ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train2017/000000196888</td>\n",
       "      <td>สีน้ำตาลตัวเล็กกำลังกินอาหารอยู่บนจานกระดาษสีข...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train2017/000000196888</td>\n",
       "      <td>นกน้อยตัวหนึ่งกำลังจิกกินเศษอาหารที่วางทิ้งไว้...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360211</th>\n",
       "      <td>val2017/000000338219</td>\n",
       "      <td>รถจักรยานยนต์จำนวนมากที่จอดอยู่ตรงบริเวณพื้นที...</td>\n",
       "      <td>119285</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360212</th>\n",
       "      <td>val2017/000000338219</td>\n",
       "      <td>รถมอเตอร์ไซค์จอดอยู่บนพื้นหญ้าหลายคัน ด้านหลัง...</td>\n",
       "      <td>119285</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360213</th>\n",
       "      <td>val2017/000000376093</td>\n",
       "      <td>ผู้หญิงใส่เสื้อสีม่วงยืนอยู่ข้างกับผู้ชายใส่เส...</td>\n",
       "      <td>119286</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360214</th>\n",
       "      <td>val2017/000000376093</td>\n",
       "      <td>คนที่เทน้ำใส่แก้วอยู่บนโต๊ะมีพิซซาวางอยู่บนโต๊...</td>\n",
       "      <td>119286</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360215</th>\n",
       "      <td>val2017/000000376093</td>\n",
       "      <td>ผู้ชาย 2 คน ถือเหยือกรินน้ำใส่แก้ว ข้าง ๆ มีผู...</td>\n",
       "      <td>119286</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360216 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         image  \\\n",
       "0       train2017/000000373716   \n",
       "1       train2017/000000373716   \n",
       "2       train2017/000000373716   \n",
       "3       train2017/000000196888   \n",
       "4       train2017/000000196888   \n",
       "...                        ...   \n",
       "360211    val2017/000000338219   \n",
       "360212    val2017/000000338219   \n",
       "360213    val2017/000000376093   \n",
       "360214    val2017/000000376093   \n",
       "360215    val2017/000000376093   \n",
       "\n",
       "                                                 captions  split_id  split  \n",
       "0       ผู้หญิงสวมเสื้อแขนยาวสีขาวและเด็กนั่งเล่นกับสุ...         0  train  \n",
       "1       สาวคนนึงกำลังพาเด็กมานั่งเล่นอยู่ภายในสนามหญ้า...         0  train  \n",
       "2       ภาพขาวดำ ผู้หญิงนั่งบนพื้นอุ้มเด็กบนตัก ข้าง ๆ...         0  train  \n",
       "3       สีน้ำตาลตัวเล็กกำลังกินอาหารอยู่บนจานกระดาษสีข...         1  train  \n",
       "4       นกน้อยตัวหนึ่งกำลังจิกกินเศษอาหารที่วางทิ้งไว้...         1  train  \n",
       "...                                                   ...       ...    ...  \n",
       "360211  รถจักรยานยนต์จำนวนมากที่จอดอยู่ตรงบริเวณพื้นที...    119285    val  \n",
       "360212  รถมอเตอร์ไซค์จอดอยู่บนพื้นหญ้าหลายคัน ด้านหลัง...    119285    val  \n",
       "360213  ผู้หญิงใส่เสื้อสีม่วงยืนอยู่ข้างกับผู้ชายใส่เส...    119286    val  \n",
       "360214  คนที่เทน้ำใส่แก้วอยู่บนโต๊ะมีพิซซาวางอยู่บนโต๊...    119286    val  \n",
       "360215  ผู้ชาย 2 คน ถือเหยือกรินน้ำใส่แก้ว ข้าง ๆ มีผู...    119286    val  \n",
       "\n",
       "[360216 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels #+ Display data in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ผู้หญิงสวมเสื้อแขนยาวสีขาวและเด็กนั่งเล่นกับสุ...\n",
       "1         สาวคนนึงกำลังพาเด็กมานั่งเล่นอยู่ภายในสนามหญ้า...\n",
       "2         ภาพขาวดำ ผู้หญิงนั่งบนพื้นอุ้มเด็กบนตัก ข้าง ๆ...\n",
       "3         สีน้ำตาลตัวเล็กกำลังกินอาหารอยู่บนจานกระดาษสีข...\n",
       "4         นกน้อยตัวหนึ่งกำลังจิกกินเศษอาหารที่วางทิ้งไว้...\n",
       "                                ...                        \n",
       "345157    แมว 2 ตัวที่อยู่ในรถของเจ้าของจอดอยู่ตรงพื้นที...\n",
       "345158    แมว 2 ตัว นั่งอยู่ในรถยนต์ ด้านนอกมีภูเขาและท้...\n",
       "345159    รถยนต์คันสีดำอยู่ข้างกับเรือลำสีแดงใกล้จะกลับส...\n",
       "345160    คนจำนวนหนึ่งที่กำลังเดินข้ามสะพานอยู่ที่สวนสาธ...\n",
       "345161    รถสองคันจอดอยู่ใต้สะพานที่มีน้ำเล็กน้อย มีเรือ...\n",
       "Name: captions, Length: 345044, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_labels = labels[labels['split'] == \"train\" ]['captions']\n",
    "select_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'text'],\n",
       "        num_rows: 69008\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'text'],\n",
       "        num_rows: 276036\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_dict({\"image\": images, \"text\": select_labels})\n",
    "dataset = dataset.train_test_split(test_size=0.8, seed=42)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ImageCaptioningDataset(Dataset):\n",
    "    def __init__(self, dataset, processor):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        im = Image.open(item['image'])\n",
    "        encoding = self.processor(text=item['text'], images=im, padding=\"max_length\", return_tensors=\"pt\", max_length=40)\n",
    "        # remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding[\"text\"] = item[\"text\"]\n",
    "        return encoding\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     # pad the input_ids and attention_mask\n",
    "#     processed_batch = {}\n",
    "#     for key in batch[0].keys():\n",
    "#         if key != \"text\":\n",
    "#             processed_batch[key] = torch.stack([example[key] for example in batch])\n",
    "#         else:\n",
    "#             text_inputs = processor.tokenizer(\n",
    "#                 [example[\"text\"] for example in batch], padding=True, return_tensors=\"pt\"\n",
    "#             )\n",
    "#             processed_batch[\"input_ids\"] = text_inputs[\"input_ids\"]\n",
    "#             processed_batch[\"attention_mask\"] = text_inputs[\"attention_mask\"]\n",
    "#     return processed_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type llava_llama to instantiate a model of type llava_next. This is not supported for all configurations of models and can yield errors.\n",
      "Loading checkpoint shards: 100%|██████████| 9/9 [00:01<00:00,  6.03it/s]\n",
      "Some weights of LlavaNextForConditionalGeneration were not initialized from the model checkpoint at xtuner/llava-llama-3-8b-v1_1-hf and are newly initialized: ['model.image_newline', 'model.language_model.lm_head.weight', 'model.language_model.model.embed_tokens.weight', 'model.language_model.model.layers.0.input_layernorm.weight', 'model.language_model.model.layers.0.mlp.down_proj.weight', 'model.language_model.model.layers.0.mlp.gate_proj.weight', 'model.language_model.model.layers.0.mlp.up_proj.weight', 'model.language_model.model.layers.0.post_attention_layernorm.weight', 'model.language_model.model.layers.0.self_attn.k_proj.weight', 'model.language_model.model.layers.0.self_attn.o_proj.weight', 'model.language_model.model.layers.0.self_attn.q_proj.weight', 'model.language_model.model.layers.0.self_attn.v_proj.weight', 'model.language_model.model.layers.1.input_layernorm.weight', 'model.language_model.model.layers.1.mlp.down_proj.weight', 'model.language_model.model.layers.1.mlp.gate_proj.weight', 'model.language_model.model.layers.1.mlp.up_proj.weight', 'model.language_model.model.layers.1.post_attention_layernorm.weight', 'model.language_model.model.layers.1.self_attn.k_proj.weight', 'model.language_model.model.layers.1.self_attn.o_proj.weight', 'model.language_model.model.layers.1.self_attn.q_proj.weight', 'model.language_model.model.layers.1.self_attn.v_proj.weight', 'model.language_model.model.layers.10.input_layernorm.weight', 'model.language_model.model.layers.10.mlp.down_proj.weight', 'model.language_model.model.layers.10.mlp.gate_proj.weight', 'model.language_model.model.layers.10.mlp.up_proj.weight', 'model.language_model.model.layers.10.post_attention_layernorm.weight', 'model.language_model.model.layers.10.self_attn.k_proj.weight', 'model.language_model.model.layers.10.self_attn.o_proj.weight', 'model.language_model.model.layers.10.self_attn.q_proj.weight', 'model.language_model.model.layers.10.self_attn.v_proj.weight', 'model.language_model.model.layers.11.input_layernorm.weight', 'model.language_model.model.layers.11.mlp.down_proj.weight', 'model.language_model.model.layers.11.mlp.gate_proj.weight', 'model.language_model.model.layers.11.mlp.up_proj.weight', 'model.language_model.model.layers.11.post_attention_layernorm.weight', 'model.language_model.model.layers.11.self_attn.k_proj.weight', 'model.language_model.model.layers.11.self_attn.o_proj.weight', 'model.language_model.model.layers.11.self_attn.q_proj.weight', 'model.language_model.model.layers.11.self_attn.v_proj.weight', 'model.language_model.model.layers.12.input_layernorm.weight', 'model.language_model.model.layers.12.mlp.down_proj.weight', 'model.language_model.model.layers.12.mlp.gate_proj.weight', 'model.language_model.model.layers.12.mlp.up_proj.weight', 'model.language_model.model.layers.12.post_attention_layernorm.weight', 'model.language_model.model.layers.12.self_attn.k_proj.weight', 'model.language_model.model.layers.12.self_attn.o_proj.weight', 'model.language_model.model.layers.12.self_attn.q_proj.weight', 'model.language_model.model.layers.12.self_attn.v_proj.weight', 'model.language_model.model.layers.13.input_layernorm.weight', 'model.language_model.model.layers.13.mlp.down_proj.weight', 'model.language_model.model.layers.13.mlp.gate_proj.weight', 'model.language_model.model.layers.13.mlp.up_proj.weight', 'model.language_model.model.layers.13.post_attention_layernorm.weight', 'model.language_model.model.layers.13.self_attn.k_proj.weight', 'model.language_model.model.layers.13.self_attn.o_proj.weight', 'model.language_model.model.layers.13.self_attn.q_proj.weight', 'model.language_model.model.layers.13.self_attn.v_proj.weight', 'model.language_model.model.layers.14.input_layernorm.weight', 'model.language_model.model.layers.14.mlp.down_proj.weight', 'model.language_model.model.layers.14.mlp.gate_proj.weight', 'model.language_model.model.layers.14.mlp.up_proj.weight', 'model.language_model.model.layers.14.post_attention_layernorm.weight', 'model.language_model.model.layers.14.self_attn.k_proj.weight', 'model.language_model.model.layers.14.self_attn.o_proj.weight', 'model.language_model.model.layers.14.self_attn.q_proj.weight', 'model.language_model.model.layers.14.self_attn.v_proj.weight', 'model.language_model.model.layers.15.input_layernorm.weight', 'model.language_model.model.layers.15.mlp.down_proj.weight', 'model.language_model.model.layers.15.mlp.gate_proj.weight', 'model.language_model.model.layers.15.mlp.up_proj.weight', 'model.language_model.model.layers.15.post_attention_layernorm.weight', 'model.language_model.model.layers.15.self_attn.k_proj.weight', 'model.language_model.model.layers.15.self_attn.o_proj.weight', 'model.language_model.model.layers.15.self_attn.q_proj.weight', 'model.language_model.model.layers.15.self_attn.v_proj.weight', 'model.language_model.model.layers.16.input_layernorm.weight', 'model.language_model.model.layers.16.mlp.down_proj.weight', 'model.language_model.model.layers.16.mlp.gate_proj.weight', 'model.language_model.model.layers.16.mlp.up_proj.weight', 'model.language_model.model.layers.16.post_attention_layernorm.weight', 'model.language_model.model.layers.16.self_attn.k_proj.weight', 'model.language_model.model.layers.16.self_attn.o_proj.weight', 'model.language_model.model.layers.16.self_attn.q_proj.weight', 'model.language_model.model.layers.16.self_attn.v_proj.weight', 'model.language_model.model.layers.17.input_layernorm.weight', 'model.language_model.model.layers.17.mlp.down_proj.weight', 'model.language_model.model.layers.17.mlp.gate_proj.weight', 'model.language_model.model.layers.17.mlp.up_proj.weight', 'model.language_model.model.layers.17.post_attention_layernorm.weight', 'model.language_model.model.layers.17.self_attn.k_proj.weight', 'model.language_model.model.layers.17.self_attn.o_proj.weight', 'model.language_model.model.layers.17.self_attn.q_proj.weight', 'model.language_model.model.layers.17.self_attn.v_proj.weight', 'model.language_model.model.layers.18.input_layernorm.weight', 'model.language_model.model.layers.18.mlp.down_proj.weight', 'model.language_model.model.layers.18.mlp.gate_proj.weight', 'model.language_model.model.layers.18.mlp.up_proj.weight', 'model.language_model.model.layers.18.post_attention_layernorm.weight', 'model.language_model.model.layers.18.self_attn.k_proj.weight', 'model.language_model.model.layers.18.self_attn.o_proj.weight', 'model.language_model.model.layers.18.self_attn.q_proj.weight', 'model.language_model.model.layers.18.self_attn.v_proj.weight', 'model.language_model.model.layers.19.input_layernorm.weight', 'model.language_model.model.layers.19.mlp.down_proj.weight', 'model.language_model.model.layers.19.mlp.gate_proj.weight', 'model.language_model.model.layers.19.mlp.up_proj.weight', 'model.language_model.model.layers.19.post_attention_layernorm.weight', 'model.language_model.model.layers.19.self_attn.k_proj.weight', 'model.language_model.model.layers.19.self_attn.o_proj.weight', 'model.language_model.model.layers.19.self_attn.q_proj.weight', 'model.language_model.model.layers.19.self_attn.v_proj.weight', 'model.language_model.model.layers.2.input_layernorm.weight', 'model.language_model.model.layers.2.mlp.down_proj.weight', 'model.language_model.model.layers.2.mlp.gate_proj.weight', 'model.language_model.model.layers.2.mlp.up_proj.weight', 'model.language_model.model.layers.2.post_attention_layernorm.weight', 'model.language_model.model.layers.2.self_attn.k_proj.weight', 'model.language_model.model.layers.2.self_attn.o_proj.weight', 'model.language_model.model.layers.2.self_attn.q_proj.weight', 'model.language_model.model.layers.2.self_attn.v_proj.weight', 'model.language_model.model.layers.20.input_layernorm.weight', 'model.language_model.model.layers.20.mlp.down_proj.weight', 'model.language_model.model.layers.20.mlp.gate_proj.weight', 'model.language_model.model.layers.20.mlp.up_proj.weight', 'model.language_model.model.layers.20.post_attention_layernorm.weight', 'model.language_model.model.layers.20.self_attn.k_proj.weight', 'model.language_model.model.layers.20.self_attn.o_proj.weight', 'model.language_model.model.layers.20.self_attn.q_proj.weight', 'model.language_model.model.layers.20.self_attn.v_proj.weight', 'model.language_model.model.layers.21.input_layernorm.weight', 'model.language_model.model.layers.21.mlp.down_proj.weight', 'model.language_model.model.layers.21.mlp.gate_proj.weight', 'model.language_model.model.layers.21.mlp.up_proj.weight', 'model.language_model.model.layers.21.post_attention_layernorm.weight', 'model.language_model.model.layers.21.self_attn.k_proj.weight', 'model.language_model.model.layers.21.self_attn.o_proj.weight', 'model.language_model.model.layers.21.self_attn.q_proj.weight', 'model.language_model.model.layers.21.self_attn.v_proj.weight', 'model.language_model.model.layers.22.input_layernorm.weight', 'model.language_model.model.layers.22.mlp.down_proj.weight', 'model.language_model.model.layers.22.mlp.gate_proj.weight', 'model.language_model.model.layers.22.mlp.up_proj.weight', 'model.language_model.model.layers.22.post_attention_layernorm.weight', 'model.language_model.model.layers.22.self_attn.k_proj.weight', 'model.language_model.model.layers.22.self_attn.o_proj.weight', 'model.language_model.model.layers.22.self_attn.q_proj.weight', 'model.language_model.model.layers.22.self_attn.v_proj.weight', 'model.language_model.model.layers.23.input_layernorm.weight', 'model.language_model.model.layers.23.mlp.down_proj.weight', 'model.language_model.model.layers.23.mlp.gate_proj.weight', 'model.language_model.model.layers.23.mlp.up_proj.weight', 'model.language_model.model.layers.23.post_attention_layernorm.weight', 'model.language_model.model.layers.23.self_attn.k_proj.weight', 'model.language_model.model.layers.23.self_attn.o_proj.weight', 'model.language_model.model.layers.23.self_attn.q_proj.weight', 'model.language_model.model.layers.23.self_attn.v_proj.weight', 'model.language_model.model.layers.24.input_layernorm.weight', 'model.language_model.model.layers.24.mlp.down_proj.weight', 'model.language_model.model.layers.24.mlp.gate_proj.weight', 'model.language_model.model.layers.24.mlp.up_proj.weight', 'model.language_model.model.layers.24.post_attention_layernorm.weight', 'model.language_model.model.layers.24.self_attn.k_proj.weight', 'model.language_model.model.layers.24.self_attn.o_proj.weight', 'model.language_model.model.layers.24.self_attn.q_proj.weight', 'model.language_model.model.layers.24.self_attn.v_proj.weight', 'model.language_model.model.layers.25.input_layernorm.weight', 'model.language_model.model.layers.25.mlp.down_proj.weight', 'model.language_model.model.layers.25.mlp.gate_proj.weight', 'model.language_model.model.layers.25.mlp.up_proj.weight', 'model.language_model.model.layers.25.post_attention_layernorm.weight', 'model.language_model.model.layers.25.self_attn.k_proj.weight', 'model.language_model.model.layers.25.self_attn.o_proj.weight', 'model.language_model.model.layers.25.self_attn.q_proj.weight', 'model.language_model.model.layers.25.self_attn.v_proj.weight', 'model.language_model.model.layers.26.input_layernorm.weight', 'model.language_model.model.layers.26.mlp.down_proj.weight', 'model.language_model.model.layers.26.mlp.gate_proj.weight', 'model.language_model.model.layers.26.mlp.up_proj.weight', 'model.language_model.model.layers.26.post_attention_layernorm.weight', 'model.language_model.model.layers.26.self_attn.k_proj.weight', 'model.language_model.model.layers.26.self_attn.o_proj.weight', 'model.language_model.model.layers.26.self_attn.q_proj.weight', 'model.language_model.model.layers.26.self_attn.v_proj.weight', 'model.language_model.model.layers.27.input_layernorm.weight', 'model.language_model.model.layers.27.mlp.down_proj.weight', 'model.language_model.model.layers.27.mlp.gate_proj.weight', 'model.language_model.model.layers.27.mlp.up_proj.weight', 'model.language_model.model.layers.27.post_attention_layernorm.weight', 'model.language_model.model.layers.27.self_attn.k_proj.weight', 'model.language_model.model.layers.27.self_attn.o_proj.weight', 'model.language_model.model.layers.27.self_attn.q_proj.weight', 'model.language_model.model.layers.27.self_attn.v_proj.weight', 'model.language_model.model.layers.28.input_layernorm.weight', 'model.language_model.model.layers.28.mlp.down_proj.weight', 'model.language_model.model.layers.28.mlp.gate_proj.weight', 'model.language_model.model.layers.28.mlp.up_proj.weight', 'model.language_model.model.layers.28.post_attention_layernorm.weight', 'model.language_model.model.layers.28.self_attn.k_proj.weight', 'model.language_model.model.layers.28.self_attn.o_proj.weight', 'model.language_model.model.layers.28.self_attn.q_proj.weight', 'model.language_model.model.layers.28.self_attn.v_proj.weight', 'model.language_model.model.layers.29.input_layernorm.weight', 'model.language_model.model.layers.29.mlp.down_proj.weight', 'model.language_model.model.layers.29.mlp.gate_proj.weight', 'model.language_model.model.layers.29.mlp.up_proj.weight', 'model.language_model.model.layers.29.post_attention_layernorm.weight', 'model.language_model.model.layers.29.self_attn.k_proj.weight', 'model.language_model.model.layers.29.self_attn.o_proj.weight', 'model.language_model.model.layers.29.self_attn.q_proj.weight', 'model.language_model.model.layers.29.self_attn.v_proj.weight', 'model.language_model.model.layers.3.input_layernorm.weight', 'model.language_model.model.layers.3.mlp.down_proj.weight', 'model.language_model.model.layers.3.mlp.gate_proj.weight', 'model.language_model.model.layers.3.mlp.up_proj.weight', 'model.language_model.model.layers.3.post_attention_layernorm.weight', 'model.language_model.model.layers.3.self_attn.k_proj.weight', 'model.language_model.model.layers.3.self_attn.o_proj.weight', 'model.language_model.model.layers.3.self_attn.q_proj.weight', 'model.language_model.model.layers.3.self_attn.v_proj.weight', 'model.language_model.model.layers.30.input_layernorm.weight', 'model.language_model.model.layers.30.mlp.down_proj.weight', 'model.language_model.model.layers.30.mlp.gate_proj.weight', 'model.language_model.model.layers.30.mlp.up_proj.weight', 'model.language_model.model.layers.30.post_attention_layernorm.weight', 'model.language_model.model.layers.30.self_attn.k_proj.weight', 'model.language_model.model.layers.30.self_attn.o_proj.weight', 'model.language_model.model.layers.30.self_attn.q_proj.weight', 'model.language_model.model.layers.30.self_attn.v_proj.weight', 'model.language_model.model.layers.31.input_layernorm.weight', 'model.language_model.model.layers.31.mlp.down_proj.weight', 'model.language_model.model.layers.31.mlp.gate_proj.weight', 'model.language_model.model.layers.31.mlp.up_proj.weight', 'model.language_model.model.layers.31.post_attention_layernorm.weight', 'model.language_model.model.layers.31.self_attn.k_proj.weight', 'model.language_model.model.layers.31.self_attn.o_proj.weight', 'model.language_model.model.layers.31.self_attn.q_proj.weight', 'model.language_model.model.layers.31.self_attn.v_proj.weight', 'model.language_model.model.layers.4.input_layernorm.weight', 'model.language_model.model.layers.4.mlp.down_proj.weight', 'model.language_model.model.layers.4.mlp.gate_proj.weight', 'model.language_model.model.layers.4.mlp.up_proj.weight', 'model.language_model.model.layers.4.post_attention_layernorm.weight', 'model.language_model.model.layers.4.self_attn.k_proj.weight', 'model.language_model.model.layers.4.self_attn.o_proj.weight', 'model.language_model.model.layers.4.self_attn.q_proj.weight', 'model.language_model.model.layers.4.self_attn.v_proj.weight', 'model.language_model.model.layers.5.input_layernorm.weight', 'model.language_model.model.layers.5.mlp.down_proj.weight', 'model.language_model.model.layers.5.mlp.gate_proj.weight', 'model.language_model.model.layers.5.mlp.up_proj.weight', 'model.language_model.model.layers.5.post_attention_layernorm.weight', 'model.language_model.model.layers.5.self_attn.k_proj.weight', 'model.language_model.model.layers.5.self_attn.o_proj.weight', 'model.language_model.model.layers.5.self_attn.q_proj.weight', 'model.language_model.model.layers.5.self_attn.v_proj.weight', 'model.language_model.model.layers.6.input_layernorm.weight', 'model.language_model.model.layers.6.mlp.down_proj.weight', 'model.language_model.model.layers.6.mlp.gate_proj.weight', 'model.language_model.model.layers.6.mlp.up_proj.weight', 'model.language_model.model.layers.6.post_attention_layernorm.weight', 'model.language_model.model.layers.6.self_attn.k_proj.weight', 'model.language_model.model.layers.6.self_attn.o_proj.weight', 'model.language_model.model.layers.6.self_attn.q_proj.weight', 'model.language_model.model.layers.6.self_attn.v_proj.weight', 'model.language_model.model.layers.7.input_layernorm.weight', 'model.language_model.model.layers.7.mlp.down_proj.weight', 'model.language_model.model.layers.7.mlp.gate_proj.weight', 'model.language_model.model.layers.7.mlp.up_proj.weight', 'model.language_model.model.layers.7.post_attention_layernorm.weight', 'model.language_model.model.layers.7.self_attn.k_proj.weight', 'model.language_model.model.layers.7.self_attn.o_proj.weight', 'model.language_model.model.layers.7.self_attn.q_proj.weight', 'model.language_model.model.layers.7.self_attn.v_proj.weight', 'model.language_model.model.layers.8.input_layernorm.weight', 'model.language_model.model.layers.8.mlp.down_proj.weight', 'model.language_model.model.layers.8.mlp.gate_proj.weight', 'model.language_model.model.layers.8.mlp.up_proj.weight', 'model.language_model.model.layers.8.post_attention_layernorm.weight', 'model.language_model.model.layers.8.self_attn.k_proj.weight', 'model.language_model.model.layers.8.self_attn.o_proj.weight', 'model.language_model.model.layers.8.self_attn.q_proj.weight', 'model.language_model.model.layers.8.self_attn.v_proj.weight', 'model.language_model.model.layers.9.input_layernorm.weight', 'model.language_model.model.layers.9.mlp.down_proj.weight', 'model.language_model.model.layers.9.mlp.gate_proj.weight', 'model.language_model.model.layers.9.mlp.up_proj.weight', 'model.language_model.model.layers.9.post_attention_layernorm.weight', 'model.language_model.model.layers.9.self_attn.k_proj.weight', 'model.language_model.model.layers.9.self_attn.o_proj.weight', 'model.language_model.model.layers.9.self_attn.q_proj.weight', 'model.language_model.model.layers.9.self_attn.v_proj.weight', 'model.language_model.model.norm.weight', 'model.multi_modal_projector.linear_1.bias', 'model.multi_modal_projector.linear_1.weight', 'model.multi_modal_projector.linear_2.bias', 'model.multi_modal_projector.linear_2.weight', 'model.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_model.pre_layrnorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#. Init regular Blip-2 model \n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            \"xtuner/llava-llama-3-8b-v1_1-hf\",\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        ).cuda()\n",
    "processor = LlavaNextProcessor.from_pretrained(\"xtuner/llava-llama-3-8b-v1_1-hf\")\n",
    "\n",
    "#. Use Typhoon 7b as tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CamembertTokenizerFast(name_or_path='airesearch/wangchanberta-base-att-spm-uncased', vocab_size=25005, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<s>NOTUSED', '</s>NOTUSED', '<_>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<_>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t25004: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,961,472 || all params: 7,015,557,120 || trainable%: 0.1420\n"
     ]
    }
   ],
   "source": [
    "model.config.text_config.vocab_size = len(tokenizer)\n",
    "model.language_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "processor.tokenizer = tokenizer\n",
    "model.config.eos_token_id = 6\n",
    "\n",
    "# Let's define the LoraConfig\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\"]\n",
    ")\n",
    "\n",
    "# Parameter effective fine-tuning\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_dataset = ImageCaptioningDataset(dataset['train'], processor)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "test_dataset = ImageCaptioningDataset(dataset['test'], processor)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Iteration per Epoch = 17252\n"
     ]
    }
   ],
   "source": [
    "print(f\" Number of Iteration per Epoch = {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17252 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 26.00 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 23.06 GiB is allocated by PyTorch, and 161.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device, torch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     25\u001b[0m training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/peft/peft_model.py:604\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    603\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/transformers/models/llava_next/modeling_llava_next.py:503\u001b[0m, in \u001b[0;36mLlavaNextForConditionalGeneration.forward\u001b[0;34m(self, input_ids, pixel_values, image_sizes, attention_mask, position_ids, past_key_values, inputs_embeds, vision_feature_layer, vision_feature_select_strategy, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    501\u001b[0m batch_size, num_patches, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    502\u001b[0m reshaped_pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m num_patches, num_channels, height, width)\n\u001b[0;32m--> 503\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_tower\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreshaped_pixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m selected_image_feature \u001b[38;5;241m=\u001b[39m image_features\u001b[38;5;241m.\u001b[39mhidden_states[vision_feature_layer]\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vision_feature_select_strategy \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:923\u001b[0m, in \u001b[0;36mCLIPVisionModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m>>> pooled_output = outputs.pooler_output  # pooled CLS states\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m    921\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:850\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    847\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    848\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m--> 850\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    858\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:636\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    628\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    629\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    630\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m         output_attentions,\n\u001b[1;32m    634\u001b[0m     )\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 636\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    643\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    645\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:388\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    386\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    387\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm2(hidden_states)\n\u001b[0;32m--> 388\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    391\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:344\u001b[0m, in \u001b[0;36mCLIPMLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    343\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(hidden_states)\n\u001b[0;32m--> 344\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/cloud/SuperAI/env/lib/python3.11/site-packages/transformers/activations.py:96\u001b[0m, in \u001b[0;36mQuickGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.702\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 92.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 26.00 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 23.06 GiB is allocated by PyTorch, and 161.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "experiment_name = \"model_llava_LLama3\"\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=6e-4, weight_decay=1e-2)\n",
    "\n",
    "for epoch in range(2):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    model.train()\n",
    "    \n",
    "    training_loss = 0\n",
    "    validate_loss = 0\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch.pop(\"input_ids\").to(device)\n",
    "        pixel_values = batch.pop(\"pixel_values\").to(device, torch.float16)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            pixel_values=pixel_values,\n",
    "            labels=input_ids\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        training_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    print(\"Training loss\", training_loss / len(train_dataloader))\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    save_dir = os.path.join(experiment_name, str(epoch))\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    model.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlavaNextProcessor:\n",
       "- image_processor: LlavaNextImageProcessor {\n",
       "  \"_valid_processor_keys\": [\n",
       "    \"images\",\n",
       "    \"do_resize\",\n",
       "    \"size\",\n",
       "    \"resample\",\n",
       "    \"do_center_crop\",\n",
       "    \"crop_size\",\n",
       "    \"do_rescale\",\n",
       "    \"rescale_factor\",\n",
       "    \"do_normalize\",\n",
       "    \"image_mean\",\n",
       "    \"image_std\",\n",
       "    \"do_convert_rgb\",\n",
       "    \"return_tensors\",\n",
       "    \"data_format\",\n",
       "    \"input_data_format\"\n",
       "  ],\n",
       "  \"crop_size\": {\n",
       "    \"height\": 336,\n",
       "    \"width\": 336\n",
       "  },\n",
       "  \"do_center_crop\": true,\n",
       "  \"do_convert_rgb\": true,\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_grid_pinpoints\": [\n",
       "    [\n",
       "      336,\n",
       "      672\n",
       "    ],\n",
       "    [\n",
       "      672,\n",
       "      336\n",
       "    ],\n",
       "    [\n",
       "      672,\n",
       "      672\n",
       "    ],\n",
       "    [\n",
       "      1008,\n",
       "      336\n",
       "    ],\n",
       "    [\n",
       "      336,\n",
       "      1008\n",
       "    ]\n",
       "  ],\n",
       "  \"image_mean\": [\n",
       "    0.48145466,\n",
       "    0.4578275,\n",
       "    0.40821073\n",
       "  ],\n",
       "  \"image_processor_type\": \"LlavaNextImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.26862954,\n",
       "    0.26130258,\n",
       "    0.27577711\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"shortest_edge\": 336\n",
       "  }\n",
       "}\n",
       "\n",
       "- tokenizer: CamembertTokenizerFast(name_or_path='airesearch/wangchanberta-base-att-spm-uncased', vocab_size=25005, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>', 'additional_special_tokens': ['<s>NOTUSED', '</s>NOTUSED', '<_>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>NOTUSED\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t5: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t6: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t8: AddedToken(\"<_>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t25004: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"LlavaNextProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 8\n",
    "res = processor(text=dataset['train'].select([idx])['text'][0], \n",
    "          images=Image.open(dataset['train'].select([idx])['image'][0]), padding=\"max_length\", return_tensors=\"pt\", max_length=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 25]), torch.Size([1, 5, 3, 336, 336]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['input_ids'].shape, res['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "IM = Image.open(dataset['test'].select([0])['image'][0])\n",
    "LABEL = dataset['test'].select([0])['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_sizes'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor(text=LABEL, images=IM).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#. Litter bit test\n",
    "im = dataset['test'].select([500,501])['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = processor(im, return_tensors=\"pt\").to(device, torch.float16).pixel_values\n",
    "model.eval()\n",
    "\n",
    "outputs = model.generate(pixel_values=pixel_values)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_caption = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(type(generated_caption),generated_caption[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/model/model_blip2(F)_Typhoon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load image to List: 100%|██████████| 40670/40670 [00:03<00:00, 12160.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row in submission : 40670\n"
     ]
    }
   ],
   "source": [
    "# Load data to test 2 path\n",
    "#? 1. test/food/\n",
    "#? 2. test/travel/\n",
    "#? 3. test2017/\n",
    "\n",
    "import datasets\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "images_test = []\n",
    "\n",
    "image1_path = list(Path(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test2017\").glob(\"*.jpg\"))\n",
    "image2_path = list(Path(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test_kaggle/food\").glob(\"*.jpg\"))\n",
    "image3_path = list(Path(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/data/test_kaggle/travel\").glob(\"*.jpg\"))\n",
    "\n",
    "image1_path.extend(image2_path)\n",
    "image1_path.extend(image3_path)\n",
    "\n",
    "merge_path = image1_path\n",
    "data_size = len(merge_path)\n",
    "\n",
    "\n",
    "for i in tqdm(image1_path, desc=\"Load image to List\"):\n",
    "    images_test.append(Image.open(i))\n",
    "\n",
    "data_testset = datasets.Dataset.from_dict({\"image\": images_test })\n",
    "\n",
    "print(f\"row in submission : {data_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.26s/it]\n",
      "2024-04-23 22:45:14.198929: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-23 22:45:14.688201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model_load = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "            \"xtuner/llava-llama-3-8b-v1_1-hf\",\n",
    "            load_in_8bit=True,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "processor = LlavaNextProcessor.from_pretrained(\"xtuner/llava-llama-3-8b-v1_1-hf\")\n",
    "\n",
    "#. Use Typhoon 7b as tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"airesearch/wangchanberta-base-att-spm-uncased\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "model_load.config.text_config.vocab_size = len(tokenizer)\n",
    "model_load.language_model.resize_token_embeddings(len(tokenizer))\n",
    "processor.tokenizer = tokenizer\n",
    "\n",
    "#. Set ID Token\n",
    "model_load.config.eos_token_id = 6\n",
    "\n",
    "#. Adapter path\n",
    "model_load.load_adapter(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/model/model_blip2(F)_Typhoon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "\n",
    "for i in tqdm(range(0,data_size,40), desc=\"Genarated Answer Vector : \"):\n",
    "    im = data_testset.select([i])['image'][0]\n",
    "    pixel_values = processor(im, return_tensors=\"pt\").to(device, torch.float16).pixel_values\n",
    "    model_load.eval()\n",
    "    outputs.append(model_load.generate(pixel_values=pixel_values))\n",
    "\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_caption = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/resource/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 244/244 [47:44<00:00, 11.74s/it]\n"
     ]
    }
   ],
   "source": [
    "test_images = submission.image_id.tolist()\n",
    "test_images = [\"data/\"+im + \".jpg\" for im in test_images]\n",
    "\n",
    "\n",
    "batch = 200\n",
    "test_prediction = []\n",
    "\n",
    "for i in tqdm(range(0, len(test_images), batch)):\n",
    "    images = [Image.open(im) for im in test_images[i: i+batch]]\n",
    "    pixel_values = processor(images, return_tensors=\"pt\").to(\"cuda\", torch.float16).pixel_values\n",
    "\n",
    "    outputs = model_load.generate(pixel_values=pixel_values)\n",
    "    generated_caption = processor.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    test_prediction.extend(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['คนคนคนกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังอยู่ตรงถนนมีคนกำลังอยู่ตรงถนน',\n",
       " 'ที่กำลังสีเหลืองอยู่บนพื้น',\n",
       " 'คนคนกำลังอยู่บนโต๊ะสีเขียววางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงสนามหญ้า',\n",
       " 'แมวสีน้ำตาลกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังวางอยู่บนโต๊ะ',\n",
       " 'จอดอยู่บนถนนมีต้นไม้สีเขียวจอดอยู่บนถนน',\n",
       " 'คนคนกำลังสีแดงกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนกำลังกำลังกำลังอยู่ตรง Насеคนกำลังอยู่',\n",
       " 'ราฟราฟสีเขียวกำลังยืนอยู่บนพื้นพื้นพื้นพื้นพื้นพื้น',\n",
       " 'ขนมปังสีน้ำตาลสีเหลืองและสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังกำลังกำลังสีน้ำตาลสีน้ำตาลและสีแดง',\n",
       " '2 คนกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนหนึ่งกำลังเล่นเทนนิส',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงห้อง',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'ที่จอดอยู่บนถนนมีต้นไม้อยู่บนถนน',\n",
       " 'คนคนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'photoวมเสื้อสีเขียวสีขาวสีขาวกำลังสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่ตรงล่างล่างล่างล่างล่างล่างล่างล่างล่างล่าง',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'จอดอยู่บนถนนมีต้นไม้อยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังเล่นอยู่บนถนน',\n",
       " 'จอดอยู่บนถนนมีคนจอดอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังเล่นสกีสกีสกีสกีสกีสกีสก',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนกำลังเล่นเบสบอล',\n",
       " 'คนคนกำลังสีแดงกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'photoวมเสื้อสีเขียวสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนามหญ้า',\n",
       " 'คนกำลังกำลังกำลังอยู่บนเตียงสีเขียว',\n",
       " 'คนคนกำลังกำลังอยู่ในห้องครัวมีคนกำลังอยู่ในห้อง',\n",
       " 'คนคนคนกำลังเล่นสวมเสื้อสีน้ำเงินอยู่บนพื้นาหลสีน้ำเงิน',\n",
       " 'คนคนคนคนคนคนคนกำลังอยู่ในสนาม',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังเล่นสเก็ตบอร์ดอยู่ตรงสนาม',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนพื้นพื้น',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังคนคนคนคนคนคน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'ที่กำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังอยู่ในสนามหญ้า',\n",
       " '2 ตัวกำลังกำลังกำลังยืนอยู่ตรงสีน้ำตาล',\n",
       " 'คนคนคนกำลังเล่นกีฬาสีเขียวอยู่ตรงสนาม',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนสนามหญ้า',\n",
       " 'คนคนกำลังเล่นเบสบอลอยู่ในสนามเบสบอล',\n",
       " 'ที่กำลังสีเหลืองสีเหลืองสีเหลืองอยู่บนโต๊ะสีน้ำตาลสีเขียว',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่บนพื้นพื้นพื้นพื้น',\n",
       " 'ขนมปังสีน้ำตาลสีน้ำตาลสีน้ำตาลและสีน้ำตาลอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนคนคนกำลังคนคนกำลังอยู่ในห้อง',\n",
       " 'คนคนกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลและสีน้ำตาลและสีน้ำตาลสีน้ำตาล',\n",
       " 'ที่กำลังสีเหลืองสีเหลืองสีเหลืองและสีเหลืองสีเหลืองอยู่บนโต๊ะสีเขียว',\n",
       " 'คนคนคนคนคนคนกำลังกำลังสีแดง',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นอยู่บนพื้นพื้น',\n",
       " 'ที่กำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนกำลังเล่นเล่นเล่นเล่น',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังสีเขียวสีเขียวกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรง',\n",
       " 'คนคนกำลังเล่นสกีิมะสีเขียวอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังสีเขียวสีเขียว',\n",
       " 'จอดอยู่บนถนนมีรถรถรถรถรถรถรถรถอยู่บนถนน',\n",
       " 'คนคนกำลังเล่นเบสบอลอยู่ตรงสนาม',\n",
       " 'คนคนคนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังสีเหลือง',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงพื้น',\n",
       " 'photoวมเสื้อสีเหลืองกำลังสีน้ำตาลสีแดงกำลังสีเขียวและสีเหลืองและสี',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังยืนอยู่ตรงสนามหญ้า',\n",
       " 'ราฟราฟสีเขียวกำลังยืนอยู่ตรงพื้นพื้นพื้นพื้นพื้น',\n",
       " '2 ยี 1 ตัวกำลังกำลังยืนอยู่ตรง',\n",
       " 'คนคนคนคนคนกำลังเล่นเทนนิสอยู่ในสนามเทนนิส',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงห้อง',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังสีแดง',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนกำลังเล่นเล่น',\n",
       " 'บนโต๊ะมีจานสีเหลืองมีสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนกำลังเล่นเบสบอลอยู่ตรงสนามเบสบอล',\n",
       " 'คนคนคนคนคนคนกำลังคนคนกำลังคนกำลังอยู่ในสนาม',\n",
       " 'ที่กำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังสีเหลืองกำลังอยู่ในสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนคนคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีเหลืองและสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'สำหรับการสำหรับการสีเขียวกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนกำลังเล่นเล่นเล่นเล่นอยู่ตรงสนาม',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'photophotophotophotophotophotophotoวมเสื้อสีฟ้าสีเหลืองกำลังอยู่บนจาน',\n",
       " 'คนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนกำลังคนกำลังอยู่บนสนามหญ้า',\n",
       " 'คนคนกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนกำลังอยู่ในสนามหญ้าสีเขียว',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังสีน้ำตาลวางอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังสีแดงกำลังสีแดงกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนกำลังกำลังอยู่บนพื้นพื้น',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังสีแดงสีน้ำตาลสีแดงสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนกำลังเล่นสวมเสื้อสีแดงกำลังเล่นสเกตบอร์ดอยู่',\n",
       " 'คนคนกำลังกำลังสีแดงสีน้ำตาลสีน้ำตาลและสีน้ำตาลสีน้ำตาลสีน้ำตาลอยู่',\n",
       " 'คนกำลังสีเหลืองสีเหลืองกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังอยู่ในถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนพื้นพื้นพื้น',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนกำลังสีเหลืองกำลังกำลังสีเหลืองและสีเหลือง',\n",
       " 'คนคนคนคนคนกำลังคนกำลังอยู่ในสนาม',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนกำลังคนกำลังคนคนกำลังคนคนกำลัง',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนกำลังเล่นเล่นอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังสีน้ำตาลสีเหลืองและสีเหลืองและสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนกำลังอยู่ในสีน้ำตาลสีแดงสีเหลืองสีเหลืองสีแดงสีแดง',\n",
       " 'คนคนคนคนกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนกำลังอยู่ในสนามหญ้า',\n",
       " 'ตัวตัวตัวตัวตัวกำลังอยู่บนพื้นพื้น',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่ตรงสนามหญ้า',\n",
       " 'จอดอยู่บนถนนมีต้นไม้อยู่บนถนน',\n",
       " 'ไฟสีเหลืองกำลังสีเหลืองสีแดงกำลังสีเขียวสีเขียวกำลังสีเขียวสีเขียว',\n",
       " 'ที่กำลังอยู่ในถนน',\n",
       " 'คนคนคนคนคนคนคนกำลังคนคนคนกำลังอยู่บนพื้นสนาม',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'photo난สีน้ำตาลสีเหลืองสีน้ำตาลสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'ไฟสีเหลืองกำลังสีเหลืองกำลังอยู่ตรงรถรถรถรถรถรถรถรถสี',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนหนึ่งกำลังอยู่บนสนามหญ้า',\n",
       " 'คนคนคนคนคนกำลังอยู่บนสนามหญ้า',\n",
       " 'จอดอยู่บนถนนมีต้นไม้สีเขียวและสีเขียว',\n",
       " 'คนคนสีเหลืองกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'ไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟ',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'photoวมเสื้อสีเหลืองสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'ราฟราฟสีเขียวกำลังอยู่ตรงถนน',\n",
       " 'photo난สีเหลืองสีเหลืองกำลังสีเหลืองสีเหลืองสีเหลืองสีเหลืองวางอยู่บน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'สำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการสำหรับการ',\n",
       " 'คนคนกำลังสีเขียวกำลังกำลังกำลังสีเขียวกำลังอยู่ตรงสีเขียว',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนพื้นพื้นพื้นพื้นพื้นมีต้นไม้',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังกำลังอยู่ตรงตรงตรงสีเขียวอยู่ตรงล่างสีเขียว',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนกำลังสีเหลืองและสีเหลืองกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังเล่นสกีาหลสีเขียวอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'จอดอยู่บนถนนมีต้นไม้อยู่บนถนน',\n",
       " 'คนคนคนคนคนคนกำลังคนคนกำลังคนกำลังคนคนคนคนกำลังคน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่บนพื้น',\n",
       " 'photoวมเสื้อสีเหลืองสีน้ำตาลกำลังวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังเล่นสกีาหลสีเขียวอยู่บนสนามหญ้า',\n",
       " 'คนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นคนคนกำลังเล่นอยู่ตรงสนาม',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนกำลังเล่นคน',\n",
       " 'คนคนคนกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนกำลังเล่นสเกตบอร์ดอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังจอดอยู่บนถนน',\n",
       " 'จอดอยู่บนถนนมีต้นไม้สีเขียวจอดอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังอยู่ในสนาม',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสกีสกีสกีสกีสกี',\n",
       " 'คนคนคนคนคนกำลังเล่นสวมเสื้อสีน้ำเงินกำลังอยู่บนพื้นพื้น',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังสีแดง',\n",
       " 'คนคนคนคนคนคนคนกำลังคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนคนคนกำลังกำลังกำลังกำลังอยู่ตรงล่าง',\n",
       " 'คนคนกำลังกำลังกำลังสีเหลืองกำลังสีเหลือง',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'จอดอยู่ตรงสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลอยู่ตรงสี',\n",
       " 'ที่กำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'ไฟสีเหลืองกำลังสีน้ำตาลกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนนมีคนกำลังอยู่',\n",
       " '2 ตัวกำลังกำลังยืนอยู่ตรงสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนกำลัง',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'ขนมปังสีน้ำตาลสีน้ำตาลสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " '้าลายสีเขียวกำลังยืนอยู่บนพื้นหญ้าสีเขียว',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนกำลังกำลังกำลังเล่นสกีิมะหิมะอยู่ตรงิมะ',\n",
       " 'คนคนคนกำลังเล่นสกีหหหหหหหหหหห',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'photo난สีน้ำตาลกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนกำลังกำลังสีแดงกำลังกำลังสีแดง',\n",
       " 'คนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'บนโต๊ะมี บนโต๊ะมีปังวางอยู่บนโต๊ะ',\n",
       " 'ขนมปังสีน้ำตาลสีน้ำตาลวางอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'photo난สีเหลืองกำลังสีเหลืองสีเหลืองกำลังสีเหลืองและสีเหลืองวางอยู่บน',\n",
       " 'คนคนคนคนสวมเสื้อสีเหลืองกำลังเล่นสเกตบอร์ดอยู่ตรงสนาม',\n",
       " 'photoวมเสื้อสีเขียวกำลังสีน้ำตาลสีเขียวกำลังสีน้ำตาลสีเขียว',\n",
       " 'คนคนคนคนคนคนคนกำลังคนคนคนคนคนกำลังคนคนกำลังคน',\n",
       " 'คนคนกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนกำลังเล่นสกีสกีสกีสกีสกีสกีสก',\n",
       " 'ไฟสีเหลืองกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นเล่นสกีาหลสีดำอยู่ตรงาหลสีดำ',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนกำลังเล่นสกีหหหหหหหหหห',\n",
       " 'คนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'จอดอยู่บนถนนมีต้นไม้สีเขียว',\n",
       " 'ที่กำลังสีเหลืองสีเหลืองวางอยู่บนโต๊ะสีเหลือง',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'ที่กำลังอยู่ในห้อง',\n",
       " 'photophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophoto',\n",
       " 'คนกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'จอดอยู่บนถนนมีจอดอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'ที่กำลังอยู่บนโต๊ะสีน้ำตาลสีน้ำตาลอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'ราฟราฟสีเขียวกำลังกำลังกำลังกำลังยืนอยู่ตรงสนามหญ้า',\n",
       " 'คนคนกำลังเล่นเทนนิสอยู่ในสนาม',\n",
       " 'คนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'จอดอยู่บนถนนมีต้นไม้มีต้นไม้อยู่ตรงถนน',\n",
       " 'คนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนหนึ่งกำลังอยู่บนโต๊ะมีกสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนกำลังสีเหลืองกำลังสีเหลืองกำลังสีเหลืองอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังกำลังยืนอยู่ตรงล่างล่างสีเหลือง',\n",
       " 'คนกำลังกำลังกำลังอยู่ตรงสนามหญ้า',\n",
       " 'คนคนกำลังอยู่บนถนนมีต้นไม้อยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนามหญ้า',\n",
       " 'บนโต๊ะมี규สีเหลืองมีสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่ตรงสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นเล่นเบสบอล',\n",
       " 'photoวมเสื้อสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนกำลังคนกำลังกำลังกำลังคนกำลังกำลังคนกำลังอยู่บน',\n",
       " 'คนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังยืนอยู่ตรงล่างล่างล่างล่างล่างล่างล่างสีเขียว',\n",
       " 'คนคนกำลังกำลังกำลังสีเหลืองและสีเหลืองและสีน้ำตาลและสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนคนคนคนคนกำลังคนกำลังคนคนคนคนคนคน',\n",
       " 'photoวมเสื้อสีเหลืองสีเหลืองกำลังสีเหลืองและสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนกำลังเล่นเทนนิส',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสกีสกีสกีสกีสกี',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังจอดอยู่บนถนน',\n",
       " 'ที่กำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'perceptionสีเหลืองวางอยู่บนโต๊ะมีจานสีเหลือง',\n",
       " 'photoวมเสื้อสีเทาสีเหลืองกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังอยู่ในสนามหญ้าสีเขียวอยู่บนพื้นสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนหนึ่งกำลังอยู่ในสนาม',\n",
       " 'ไฟไฟสีเหลืองกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนกำลังสีเหลืองสีเหลืองสีน้ำตาลสีน้ำตาลสีน้ำตาลสีเหลือง',\n",
       " 'คนคนคนคนคนกำลังเล่นเล่นอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังสีเหลืองสีน้ำตาลสีเหลืองอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'photophotoวมเสื้อสีเหลืองกำลังสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นสวมเสื้อสีดำอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนกำลังคนคนกำลังอยู่ตรงคนคนกำลังคน',\n",
       " 'photo난สีเหลืองวางอยู่บนโต๊ะสีเหลือง',\n",
       " 'คนคนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลวางอยู่บนโต๊ะสี',\n",
       " 'คนคนคนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'ราฟราฟสีเขียวกำลังยืนอยู่ตรงพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนคนคนกำลังกำลังเล่นอยู่ตรง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'photoวมเสื้อสีเหลืองสีเหลืองและสีเหลืองและสีเหลืองและสีเหลืองสีเหลือง',\n",
       " 'คนคนกำลังอยู่ในสีเหลืองสีเหลืองสีเหลือง',\n",
       " 'คนคนคนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังอยู่ในสนามหญ้าสีเขียวสีเขียวอยู่บนพื้นพื้นพื้น',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ในห้องครัว',\n",
       " 'คนคนคนคนคนกำลังกำลังคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'oshiสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'ที่กำลังอยู่บนโต๊ะสีน้ำตาลและสีเขียวและสีเขียว',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ในห้อง',\n",
       " 'คนคนคนกำลังเล่นสกีสกีสกีิมะอยู่บนหิมะ',\n",
       " 'คนคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังสีน้ำตาลกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนคนกำลังเล่นอยู่บนพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังคนคนคนคนคนกำลัง',\n",
       " 'ไฟสีเหลืองสีเหลืองสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'ขนมสีเขียววางอยู่บนโต๊ะสีเขียววางอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังเล่นสเกตบอร์ดอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังเล่นสกีาหลสีดำอยู่บนาหลาหลสีดำาหลสีเขียว',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังตีเทนนิส',\n",
       " 'คนคนกำลังสีเหลืองกำลังสีเหลืองและสีเหลืองวางอยู่บนจานสีเขียว',\n",
       " 'คนคนคนคนคนคนกำลังกำลังกำลังกำลังอยู่ตรงล่างล่างล่างล่างล่างล่าง',\n",
       " 'คนคนคนกำลังเล่นสกีาหลสีเหลืองอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนพื้น',\n",
       " 'คนคนคนคนคนคนคนคนคนกำลังคนคนคนกำลังคนคนคนกำลัง',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลัง',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลัง',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นอยู่ในสนามหญ้า',\n",
       " 'คนคนคนกำลังเล่นเบสบอลอยู่ในสนามเบสบอล',\n",
       " 'คนคนกำลังสีเหลืองกำลังสีเหลืองอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังเล่นสวมเสื้อสีเหลืองกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นอยู่ตรงสนาม',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังสีเหลืองและสีเหลืองและสีเหลืองและสี',\n",
       " 'คนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนคนคนกำลังเล่นอยู่ในสนามหญ้า',\n",
       " 'คนคนคนกำลังเล่นสวมเสื้อสีแดงกำลังเล่นสวมเสื้อสีเหลืองบน',\n",
       " 'ขนมปังสีน้ำตาลวางอยู่บนโต๊ะมีจานสีเหลือง',\n",
       " 'photophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophoto',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'photoวมเสื้อสีเหลืองกำลังสีเหลืองและสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'จอดอยู่บนถนนมีคนกำลังอยู่บนถนน',\n",
       " 'จอดอยู่บนถนนมีต้นไม้สีเขียวอยู่บนถนน',\n",
       " 'ต้นไม้สีเขียวต้นไม้อยู่บนถนนมีต้นไม้สีเขียว',\n",
       " 'คนคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังสีเหลืองสีแดงสีแดงและสีแดงสีแดงสีแดงอยู่บน',\n",
       " 'แม แมวสีน้ำตาลสีน้ำตาลกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'photoวมเสื้อสีเขียวสีเหลืองกำลังสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะสี',\n",
       " 'photoวมเสื้อสีเหลืองกำลังสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังเล่นเบสบอล',\n",
       " 'คนคนคนกำลังสีแดงกำลังสีเขียวกำลังอยู่ในสนามหญ้าสีเขียว',\n",
       " 'คนหนึ่งกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่ในห้อง',\n",
       " 'ไฟสีเหลืองกำลังกำลังกำลังจอดอยู่บนถนน',\n",
       " 'oshiสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสกีกางเกงเกงเกง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนกำลังกำลังกำลังอยู่บนพื้น',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนคนกำลังเล่นสเก็ตบอร์ดอยู่ในสนามหญ้า',\n",
       " 'ที่กำลังอยู่บนโต๊ะมีคนสีดำและสีเขียว',\n",
       " 'คนคนกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังสีเหลืองสีเหลืองและสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนคนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนพื้น',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่ตรงล่างล่างสีแดงสีแดง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'ไฟสีเหลืองกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'photophotoวมเสื้อสีเหลืองกำลังสีเหลืองกำลังกำลังสีเหลืองและสีเหลือง',\n",
       " 'คนกำลังสีเขียวสีแดงกำลังสีแดงอยู่บนถนน',\n",
       " 'จอดอยู่บนเตาสีดำสีดำและต้นไม้อยู่บนถนน',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'photoวมเสื้อสีเหลืองกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลและ',\n",
       " 'คนคนคนคนคนคนคนคนกำลังอยู่ในจานสีน้ำ',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนกำลังเล่นเบสบอลอยู่ตรงเบสบอล',\n",
       " 'รารารารารารารารารารารารารารารารารารา',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังสีน้ำตาลสีน้ำตาลอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนกำลังอยู่บนถนน',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'คนคนคนคนคนคนกำลังกำลังอยู่ตรงล่างล่างล่างล่างล่างล่างล่างล่าง',\n",
       " 'คนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนกำลังสีเหลืองกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนกำลังกำลังสีเหลืองและสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนคนกำลังคนคนกำลังคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังเล่นสกีาหลสีเหลืองอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนกำลังคนคนคนคนคนคนคนกำลัง',\n",
       " 'คนคนกำลังสีเหลืองสีเหลืองอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'corners corners corners cornersกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังเล่นเบสบอล',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนท้องฟ้า',\n",
       " 'photoวมเสื้อสีน้ำตาลสีน้ำตาลสีน้ำตาลกำลังกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'จอดอยู่บนถนนมีจอดอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนกำลังเล่นอยู่บนพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนคนคนคนคนคนคนกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลและสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนหนึ่งกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนคนคนกำลังคนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่ตรง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'ที่กำลังสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'ที่กำลังสีเขียวสีเขียวอยู่บนโต๊ะสีขาว',\n",
       " 'photoวมเสื้อสีเทาสีน้ำตาลสีน้ำตาลกำลังสีน้ำตาลสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'คนกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังกำลังเล่นอยู่ตรงล่างสีเขียว',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังเล่นอยู่บนพื้น',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนกำลัง',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ตรงสนาม',\n",
       " 'ตัวตัวตัวหนึ่งกำลังอยู่บนสนามหญ้า',\n",
       " 'ไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟ',\n",
       " 'คนคนคนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงสีเหลืองอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังสีแดงกำลังสีแดงกำลังสีแดงกำลังสีแดงอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังและสีน้ำตาลอยู่ในสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'photo난สีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลวางอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่ในถนน',\n",
       " 'คนคนคนคนคนคนกำลังเล่นอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'photoวมเสื้อสีน้ำตาลสีน้ำตาลสีน้ำตาลกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลอยู่บน',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนคนคนคนกำลังคนคนคนกำลังอยู่ในสนาม',\n",
       " 'คนคนกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีเขียวอยู่บนถนน',\n",
       " 'ราฟราฟสีเขียวกำลังยืนอยู่ตรงพื้นพื้น',\n",
       " 'photoวมเสื้อสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนคนคนคนคนกำลังคนคนคนกำลังยืนอยู่ตรงสีเขียว',\n",
       " 'คนคนกำลังสีเหลืองสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนกำลังสีเหลืองสีน้ำตาลวางอยู่บนโต๊ะสีเขียววางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนกำลังคนกำลังอยู่ตรง',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังอยู่ใน',\n",
       " 'คนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังเล่นสกีสกีสกีสกีสกีสกีสก',\n",
       " 'คนคนคนกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'ที่กำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังสีเขียวและคนกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังเล่นกีฬาเทนนิส',\n",
       " 'คนคนคนคนคนคนกำลังอยู่ในสนาม',\n",
       " 'ราฟราฟราฟสีเขียวกำลังอยู่บนพื้นหญ้า',\n",
       " 'คนคนคนคนกำลังเล่นสเก็ตบอร์ดอยู่ในสนาม',\n",
       " 'คนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'listsห้องครัวมีกสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'ราฟราฟสีน้ำตาลกำลังอยู่ตรงต้นไม้สีเขียว',\n",
       " 'คนคนคนคนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'รารารารารารารารารารารารารารารารารารา',\n",
       " 'คนกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังเล่นสวมเสื้อสีเหลืองกำลังเล่นสเก็ตบอร์ด',\n",
       " 'จอดอยู่บนถนนมีคนหนึ่งที่จอดอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังยืนอยู่ในห้อง',\n",
       " 'คนคนกำลังอยู่บนถนน',\n",
       " 'photo난สีเหลืองกำลังสีเหลืองและสีเหลืองและสีเหลืองอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนกำลังกำลังอยู่บนพื้นพื้นพื้นพื้น',\n",
       " 'คนคนคนคนกำลังกำลังกำลังอยู่ตรง',\n",
       " 'คนคนกำลังสีเหลืองสีเหลืองและสีเหลืองกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังสีแดงกำลังอยู่ตรงถนน',\n",
       " 'จอดอยู่บนถนนมีต้นไม้อยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงตรงตรงตรงตรงตรงตรงตรงตรง',\n",
       " 'ไฟสีเหลืองกำลังกำลังกำลังสีเหลืองกำลังสีเหลืองอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังสีเหลืองสีเหลืองสีเหลืองสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนกำลัง',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังอยู่ตรงถนนมีคนกำลังอยู่',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนกำลังกำลังกำลังอยู่บนพื้น',\n",
       " 'คนคนคนกำลังกำลังวางบนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังคนกำลังคนกำลังอยู่ในสนาม',\n",
       " 'คนคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนกำลังคนกำลังคนคนกำลังอยู่ตรงคน',\n",
       " 'คนคนคนคนคนคนกำลังกำลังคนกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนกำลังอยู่ในสนามหญ้า',\n",
       " 'photophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophoto',\n",
       " 'Whatever Whateverกำลังสีเหลืองกำลังสีแดงวางอยู่บนโต๊ะ',\n",
       " 'photoวมเสื้อสีเขียวกำลังสีน้ำตาลสีน้ำตาลและสีน้ำตาลสีเขียว',\n",
       " 'photoวมเสื้อสีเขียวกำลังสีน้ำตาลและสีน้ำตาลกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังจอดอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังที่กำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนกำลังอยู่ในสนามหญ้า',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงล่างสีเขียวสีเขียว',\n",
       " 'photoวมเสื้อสีเหลืองกำลังสีเหลืองกำลังสีเหลืองสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่ตรงล่างสีเขียว',\n",
       " 'คนคนกำลังอยู่ในสีน้ำตาลสีน้ำตาลสีแดง',\n",
       " 'Whateverสีเทาสีน้ำตาลสีน้ำตาลสีเหลืองและสีแดงวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนกำลังคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังสีแดงสีแดงและสีแดง',\n",
       " 'คนคนกำลังอยู่บนถนนมีคนอยู่บนถนน',\n",
       " 'listsห้องน้ำมีชชชชชชชชชชชชชช',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลกำลังกำลังสีน้ำตาลสีน้ำตาลอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'ที่กำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังสีเหลืองและสีเหลืองและสีแดงและสีแดงและ',\n",
       " 'Whateverสีน้ำตาลสีเหลืองวางอยู่บนโต๊ะสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'คนคนกำลังสีแดงกำลังอยู่บนพื้นพื้นพื้น',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่ตรงพื้น',\n",
       " 'photophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophoto',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังสีเหลืองและสีน้ำตาลสีน้ำตาลและสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงล่างล่างสีน้ำตาล',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'ราฟราฟราฟสีเขียวกำลังยืนอยู่ตรงพื้นหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนกำลังเล่นคนคนคนอยู่ในสนาม',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสกีอยู่บนถนน',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลและ',\n",
       " 'ที่จอดอยู่บนถนนมีต้นไม้อยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังอยู่บนถนนมีคนกำลังอยู่',\n",
       " 'คนกำลังกำลังสีน้ำตาลกำลังอยู่บนถนน',\n",
       " 'JavaJavaและสีเหลืองและสีเหลืองและสีเหลืองและสีเหลืองและสีแดงและ',\n",
       " 'คนคนกำลังสีน้ำตาลกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'ไฟสีเหลืองกำลังสีเหลืองกำลังสีเหลืองอยู่บนถนน',\n",
       " 'สำหรับการานสีเหลืองสีเหลืองสีเหลืองสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังเล่นเทนนิส',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนพื้นพื้นพื้น',\n",
       " 'คนคนคนกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนกำลังเล่นอยู่บนสนามหญ้า',\n",
       " 'ราฟราฟสีเขียวกำลังยืนอยู่ตรงสนามหญ้า',\n",
       " 'คนคนคนกำลังยืนอยู่ตรงถนน',\n",
       " 'photophotoวมเสื้อสีฟ้าสีเหลืองกำลังสีเหลืองและสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนกำลังคนกำลังคนกำลังคนกำลังคนกำลังอยู่บนเตียง',\n",
       " 'photoวมเสื้อสีเทาสีน้ำตาลสีน้ำตาลและสีน้ำตาลและสีน้ำตาลสีน้ำตาล',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'คนคนกำลังกำลังสีเหลืองสีเหลืองและสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนกำลังยืนอยู่บนพื้นหญ้า',\n",
       " 'คนคนกำลังอยู่ในสีน้ำตาลอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงห้อง',\n",
       " 'ราฟราฟสีเขียวกำลังยืนอยู่ตรงพื้นพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'ไฟสีเหลืองกำลังสีเหลืองกำลังอยู่บนถนน',\n",
       " 'ราฟราฟราฟสีเขียวยืนอยู่บนพื้นหญ้าสีเขียว',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่',\n",
       " 'photoวมเสื้อสีเหลืองกำลังสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ใน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในจานสีแดง',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนกำลังและคนกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนกำลังคนคนคนกำลังอยู่ในจานสีเขียว',\n",
       " 'คนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนาม',\n",
       " 'คนคนคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังเล่นสเก็ตบอร์ดอยู่ตรงสีดำ',\n",
       " 'คนคนกำลังสีเหลืองสีเหลืองกำลังสีเหลืองสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงล่างสีน้ำตาล',\n",
       " 'คนคนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'บนโต๊ะมีจานสีเหลืองวางอยู่บนจานสีเขียว',\n",
       " 'คนคนกำลังกำลังสีเหลืองสีเหลืองสีเหลืองอยู่บนเตียงสีเหลือง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังสีแดงกำลังสีแดงอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนกำลังเล่นสกีาหลสีแดงอยู่ในสนามหญ้า',\n",
       " 'ที่กำลังสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'คนคนกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังจอดอยู่บนถนน',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'จอดอยู่บนพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังสีเขียว',\n",
       " 'คนคนคนคนคนคนคนคนคนกำลังคนคนคนกำลังคนคนคนกำลัง',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'ราฟราฟสีเขียวกำลังอยู่ตรงต้นไม้อยู่',\n",
       " 'คนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนามหญ้า',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังคนคนคนคนกำลังคน',\n",
       " 'คนกำลังกำลังกำลังกำลังสีน้ำตาลกำลังสีน้ำตาลกำลังอยู่บนพื้น',\n",
       " 'คนคนคนคนคนกำลังคนกำลังคนกำลังกำลังอยู่ตรงคนกำลังอยู่ตรงล่าง',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังอยู่บนพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นเล่นอยู่บนพื้นพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนาม',\n",
       " 'Whatever Whateverกำลังสีเขียววางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนหนึ่งกำลังเล่นเทนนิส',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสกีสกีสกีสกีาหลสี',\n",
       " 'คนคนคนคนคนกำลังเล่นเล่นเล่นอยู่บนสนามหญ้า',\n",
       " 'คนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่บนพื้น',\n",
       " 'คนคนคนกำลังเล่นสกีาหลสีดำอยู่บนาหลสีเขียว',\n",
       " 'คนกำลังกำลังกำลังกำลังยืนอยู่ตรงสนามหญ้า',\n",
       " 'คนคนคนคนคนคนกำลังคนกำลังคนกำลังอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังอยู่บนโต๊ะมีคนวางอยู่',\n",
       " 'คนคนคนคนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังคนกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนคนกำลังกำลังสีเหลืองกำลังสีแดง',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นอยู่ตรงสนาม',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรง',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังอยู่ในสนาม',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " '้าสีแดงสีแดงกำลังกำลังกำลังสีเขียวกำลังสีแดงสีแดงสีแดง',\n",
       " 'คนกำลังสีเหลืองสีเหลืองอยู่บนถนน',\n",
       " 'ราฟราฟสีเขียวกำลังอยู่บนพื้นหญ้าสีเขียว',\n",
       " 'คนคนคนคนกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังสีแดงและสีน้ำตาลสีแดงและสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'ราฟราฟสีเขียวกำลังอยู่ตรงต้นไม้',\n",
       " 'คนคนคนกำลังอยู่บนโต๊ะมีคนกำลังอยู่',\n",
       " 'คนคนกำลังกำลังเล่นสกีิมะอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่ในห้อง',\n",
       " 'photo난สีเหลืองและสีเหลืองและสีเหลืองและสีเหลืองและสีเหลืองวางอยู่',\n",
       " 'คนคนคนกำลังตีตีอลอยู่ตรงสนาม',\n",
       " 'คนคนกำลังสีเหลืองกำลังสีเหลืองและสีเหลืองและสีเหลืองอยู่บนโต๊ะ',\n",
       " 'ราฟราฟสีน้ำตาลกำลังกำลังกำลังกำลังอยู่ตรง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนกำลังเล่นเล่นอยู่ตรงถนน',\n",
       " 'บนโต๊ะมี บนโต๊ะมีสีเทามีสีเทามีสีเทา',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังเล่นกากากากากา',\n",
       " 'ごแว Exสีน้ำตาลกำลังสีน้ำตาลสีน้ำตาลสีแดงกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนสนามหญ้า',\n",
       " 'คนกำลังกำลังกำลังอยู่บนพื้นพื้นพื้น',\n",
       " 'คนคนคนคนคนคนคนคนคนคนกำลังคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในห้อง',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังอยู่ตรงล่างสีน้ำตาลอยู่ตรงล่างล่างสีน้ำตาล',\n",
       " 'Whatever Whateverสีเขียวกำลังสีเหลืองกำลังสีเหลืองกำลังสีเหลืองสีเหลืองสีแดงสี',\n",
       " 'คนคนกำลังเล่นเบสบอลอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนกำลังคน',\n",
       " 'ที่กำลังกำลังอยู่บนถนน',\n",
       " 'จอดอยู่บนโต๊ะมีสีน้ำตาลสีน้ำตาลวางอยู่บนโต๊ะ',\n",
       " 'ราฟราฟสีเขียวกำลังยืนอยู่บนพื้นพื้นพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นเบสบอล',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนกำลังกำลังสีน้ำตาลกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังจอดอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนคนคนกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนกำลังเล่นสวมเสื้อสีเหลืองกำลังเล่นสเกตบอร์ดอยู่',\n",
       " 'คนคนคนกำลังเล่นสเก็ตบอร์ดอยู่ในสนาม',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนพื้น',\n",
       " 'คนคนคนคนคนคนคนคนคนคนกำลังคนคนคนคนคนกำลังคน',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนกำลังคนกำลังคนคนกำลังคนคนคนกำลังอยู่ตรง',\n",
       " 'คนคนคนคนคนคนกำลังกำลังเล่นอยู่บนเตียง',\n",
       " 'คนกำลังกำลังอยู่บนโต๊ะมีคนวางอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังกำลังอยู่บนถนน',\n",
       " 'ไฟสีเหลืองกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนกำลังกำลังกำลังอยู่บนสนามหญ้า',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนกำลังกำลังอยู่ตรงถนนมีคนกำลังอยู่',\n",
       " 'คนคนคนกำลังจอดอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในห้อง',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นเทนนิส',\n",
       " 'คนคนคนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนกำลังเล่นเบสบอล',\n",
       " 'แมวสีน้ำตาลกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนกำลังเล่นเบสบอล',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่ตรงสนามหญ้า',\n",
       " 'ที่กำลังอยู่บนโต๊ะมีกสีเหลือง',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนกำลังสีน้ำตาลกำลังสีน้ำตาลกำลังอยู่บนสนามหญ้าสีเขียว',\n",
       " 'คนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนหนึ่งกำลังสีเหลืองสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'รารารารารารารารารารารารารารารารารารา',\n",
       " 'คนคนคนคนคนกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนกำลังเล่นเบสบอล',\n",
       " 'photoวมเสื้อสีเหลืองสีเหลืองกำลังสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังกำลังอยู่บนพื้นหญ้า',\n",
       " 'คนคนกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังอยู่ตรงถนน',\n",
       " 'photoวมเสื้อสีเหลืองสีน้ำตาลสีเหลืองกำลังสีน้ำตาลสีน้ำตาลสีเหลืองอยู่บน',\n",
       " 'คนคนกำลังอยู่บนถนน',\n",
       " 'photophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophotophoto',\n",
       " 'คนคนคนคนคนคนคนกำลังกำลังคนคนคนคนกำลังคนคนคนกำลัง',\n",
       " 'คนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนกำลังกำลังกำลังอยู่ตรงถนนมีคนกำลังอยู่ตรงถนน',\n",
       " 'ขนมสีเหลืองสีเหลืองวางอยู่บนโต๊ะสีเขียววางอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'ขนมปังสีน้ำตาลสีน้ำตาลวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังเล่นสเกตบอร์ดอยู่ตรงกระกระกระกระกระกระกระกระ',\n",
       " 'ข้างมีสีเหลืองสีเหลืองกำลังสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลและสีน้ำตาลและสีน้ำตาลและ',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'แมวสีน้ำตาลกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'photo난สีน้ำตาลสีเหลืองกำลังสีเหลืองกำลังกำลังสีน้ำตาลวางอยู่บนจาน',\n",
       " 'คนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังอยู่บนสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนกำลังกำลังคนกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนกำลังเล่นสกีสกีสกีสกีสกี',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'ราฟราฟราฟสีเขียวยืนอยู่ตรงอยู่ตรงอยู่ตรงล่างล่างล่าง',\n",
       " 'photo난สีเขียวกำลังอยู่บนโต๊ะสีเขียว',\n",
       " 'ราฟราฟสีเขียวกำลังยืนอยู่ตรงพื้นหญ้า',\n",
       " 'คนคนคนคนคนคนกำลังเล่นเล่นอยู่ตรงตรงตรงล่างสีน้ำ',\n",
       " 'สำหรับการ*;สีเหลืองกำลังอยู่บนโต๊ะ',\n",
       " 'ไฟสีเหลืองกำลังสีเหลืองกำลังสีเหลืองกำลังสีเหลืองสีเหลืองสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนกำลังกำลังสีเหลืองกำลังสีแดงอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังสีเหลืองกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'จอดอยู่บนถนนมีต้นไม้สีเขียวอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'ขนมปังสีน้ำตาลสีน้ำตาลสีน้ำตาลวางอยู่บนโต๊ะ',\n",
       " 'photoวมเสื้อสีเทาสีน้ำตาลกำลังสีน้ำตาลและสีน้ำตาลอยู่บนโต๊ะ',\n",
       " 'listsห้องครัวมีชักโครกสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'รารารารารารารารารารารารารารารารารารา',\n",
       " 'คนคนคนกำลังกำลังกำลังสีเขียวกำลังอยู่ตรงสีเขียว',\n",
       " 'คนคนกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังสีเหลืองกำลังสีเหลืองและสีเหลืองอยู่บนพื้น',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีเหลืองกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนคนคนกำลังคนคนคนคนคนกำลังคนคนคนคน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนกำลังเล่นสวมเสื้อสีเขียวกำลังเล่นสวมเสื้อสีเขียวกำลัง',\n",
       " 'คนคนคนคนคนหนึ่งกำลังเล่นเทนนิส',\n",
       " 'ราฟสีน้ำตาลสีน้ำตาลกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนคนคนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่',\n",
       " 'ที่กำลังอยู่บนโต๊ะ',\n",
       " 'ที่กำลังอยู่บนโต๊ะมีชชชชชชชชสีเหลือง',\n",
       " 'บนโต๊ะมีจานสีเหลืองวางอยู่บนโต๊ะมีจานสีเหลือง',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนกำลังเล่นอยู่ตรงสนาม',\n",
       " 'photoวมเสื้อสีเทาสีน้ำตาลสีน้ำตาลและสีน้ำตาลวางอยู่บนโต๊ะสี',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนกำลังกำลังกำลังอยู่ตรงสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'บนโต๊ะมี บนจานสีเหลืองมีจานสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'ขนมสีเขียววางอยู่บนโต๊ะสีเขียววางอยู่บนโต๊ะ',\n",
       " 'ไฟสีเหลืองกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนคนกำลังคนคนคนกำลังคนคนคนกำลังคนคนกำลังคน',\n",
       " 'คนคนกำลังกำลังสีแดงกำลังกำลังสีแดงอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังสีสวมเสื้อสีน้ำตาล',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลอยู่',\n",
       " 'คนคนกำลังสีเหลืองกำลังสีเหลืองสีเหลืองสีเหลืองสีเหลืองสีเหลืองสีเหลือง',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังสีแดงอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'ราฟราฟสีเขียวยืนอยู่บนพื้นหญ้าสีเขียว',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนกำลังอยู่บนโต๊ะมีคนวางอยู่บนโต๊ะ',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'คนคนคนคนคนคนกำลังเล่นอยู่บนพื้นพื้น',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนกำลังคนกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังสีแดงอยู่บนถนน',\n",
       " 'photoวมเสื้อสีเขียวและสีน้ำตาลกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'ขนมสีน้ำตาลสีน้ำตาลสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'ที่กำลังอยู่ในถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังอยู่บนพื้น',\n",
       " 'แมวสีน้ำตาลกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังกำลังกำลังสีแดงสีแดงอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลัง',\n",
       " 'อิสสีดำสวมเสื้อสีฟ้ากำลังเล่นสเกตบอร์ดอยู่บนกระื่น',\n",
       " 'listsห้องครัวมีกสีขาวมีกสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาวสีขาว',\n",
       " 'คนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังอยู่ตรงล่างล่างล่างล่างล่างล่างล่างล่าง',\n",
       " 'คนกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'จอดอยู่บนสนามหญ้าสีเขียวสีเขียวและต้นไม้สีเขียวอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังเล่นอยู่ตรงถนน',\n",
       " 'คนคนกำลังเล่นเบสบอลอยู่ตรงสนามเบสบอล',\n",
       " 'คนคนกำลังเล่นสกีสกีาหลสีดำอยู่บนหาหลสีดำอยู่บนห',\n",
       " '้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า้า',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนหนึ่งกำลังเล่นเทนนิ',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนหนึ่งกำลังเล่นเทนนิส',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังอยู่บนถนน',\n",
       " 'จอดอยู่บนถนนมีต้นไม้มีต้นไม้อยู่บนถนน',\n",
       " 'จอดอยู่บนถนนมีต้นไม้สีเขียว',\n",
       " 'คนคนกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนามหญ้า',\n",
       " 'ไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟไฟ',\n",
       " 'คนคนกำลังสีแดงกำลังสีน้ำตาลสีแดงอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในสนามหญ้า',\n",
       " 'บนโต๊ะมีักสีเขียวและสีเขียวและสีเขียวและสีเขียวและสีเขียว',\n",
       " 'คนคนคนคนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังสีแดงอยู่บนถนน',\n",
       " 'คนกำลังสีน้ำตาลกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'ไฟสีน้ำตาลสีน้ำตาลสีน้ำตาลกำลังสีน้ำตาลและสีน้ำตาลและสีน้ำตาลและสี',\n",
       " 'คนคนกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'ไฟสีเหลืองกำลังสีเหลืองกำลังสีเหลืองอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในสนาม',\n",
       " 'คนคนกำลังกำลังเล่นอยู่บนเตียง',\n",
       " 'ไฟสีน้ำตาลกำลังกำลังกำลังกำลังกำลังสีเหลืองกำลังกำลังกำลังสีน้ำตาลสีน้ำตาล',\n",
       " 'สำหรับการ olสีน้ำตาลกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนกำลังอยู่ในถนน',\n",
       " 'ที่กำลังอยู่บนโต๊ะมีปังสีเขียว',\n",
       " 'บนโต๊ะมีจานสีเหลืองมีจานสีเหลือง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงสนามหญ้า',\n",
       " 'คนคนคนคนคนคนคนกำลังกำลังคนกำลังอยู่บนถนน',\n",
       " 'คนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังสีน้ำตาลกำลังกำลังสีน้ำตาลกำลังกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลอยู่',\n",
       " 'จอดอยู่บนโต๊ะมีต้นไม้สีเขียว',\n",
       " 'คนคนคนคนคนคนคนกำลังคนคนคนคนคนคนคนคนกำลังอยู่ใน',\n",
       " 'ไฟสีเหลืองสีเหลืองกำลังสีเหลืองสีเหลืองสีเหลืองสีแดงสีน้ำตาลสีเหลือง',\n",
       " 'คนคนคนกำลังเล่นสกีาหลสีเขียวอยู่บนาหลสีเขียว',\n",
       " '้าสีเขียวสีเหลืองสีเหลืองสีเหลืองสีเหลืองสีน้ำตาลสีเหลืองสีน้ำตาลสี',\n",
       " 'คนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนกำลังคนคนคนกำลังคนคนกำลังคนคนคนกำลัง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'photoวมเสื้อสีเทาสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'ราฟราฟสีเขียวกำลังยืนอยู่บนพื้นพื้นพื้นพื้นพื้นพื้น',\n",
       " 'คนคนกำลังสีแดงกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนกำลังคนกำลังคนกำลังคนคนกำลังคนคนคนคนคนกำลัง',\n",
       " 'photoวมเสื้อสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'photoวมเสื้อสีเทาสีน้ำตาลสีน้ำตาลและสีน้ำตาลและสีน้ำตาลและสี',\n",
       " 'oshiกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'บนโต๊ะมีจานสีเหลืองมีสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'ที่กำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนกำลังอยู่บนถนน',\n",
       " 'ไฟสีเหลืองกำลังกำลังอยู่บนถนน',\n",
       " 'photo난สีเขียวสีเหลืองสีเหลืองวางอยู่บนโต๊ะสีเหลืองสีเขียวสีเขียว',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'photoวมเสื้อสีเหลืองสีเหลืองกำลังสีเหลืองและสีเหลืองวางอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังอยู่บนถนน',\n",
       " 'ที่กำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนคนคนคนคนกำลังคนคนคนกำลังคนคนคนกำลัง',\n",
       " 'คนคนคนคนคนคนคนกำลังกำลังคนกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนคนคนกำลังคนกำลังคนคนกำลังคนคนกำลังคนกำลังสี',\n",
       " 'คนคนคนคนกำลังอยู่ในสนามหญ้า',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังอยู่ตรงล่างล่างล่างล่างล่างล่างล่างล่าง',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนคนคนคนกำลังคนคนคนกำลังคนคนกำลังคนคนกำลังอยู่บน',\n",
       " 'คนคนกำลังกำลังกำลังสีแดงอยู่ตรงสีเขียว',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนามหญ้า',\n",
       " '큰큰큰큰큰큰큰큰큰큰큰큰큰큰큰큰큰큰',\n",
       " 'คนคนคนคนกำลังเล่นสกีาหลสีเหลืองอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนกำลังกำลังกำลังกำลังสีเทาสีแดง',\n",
       " 'ที่กำลังอยู่บนถนน',\n",
       " 'คนคนคนคนกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ในห้อง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนกำลังสีแดงกำลังสีแดงกำลังสีแดงและสีแดงและสีแดง',\n",
       " 'คนกำลังกำลังสีน้ำตาลกำลังอยู่ตรงสีเขียว',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนกำลัง',\n",
       " 'คนคนคนกำลังเล่นสกีาหลสีเขียวอยู่ในสนามหญ้า',\n",
       " 'คนคนคนกำลังกำลังยืนอยู่บนพื้น',\n",
       " 'คนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนคนกำลังเล่นสเกตบอร์ดอยู่ในสนาม',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน',\n",
       " 'คนคนคนคนคนคนคนคนกำลังเล่นอยู่ในสนามหญ้า',\n",
       " 'จอดอยู่บนสนามหญ้าสีเขียวมีต้นไม้สีเขียวอยู่',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนคนคนคนคนคนคนคนคนคนคนคนคนกำลังคนคนคนคน',\n",
       " 'คนกำลังสีน้ำตาลกำลังอยู่บนเตียง',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังสีน้ำตาลกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังคนกำลังคนกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่ตรงล่างล่างล่างล่างล่างล่างล่างล่างล่าง',\n",
       " 'คนคนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนกำลังกำลังอยู่บนถนน',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'photophotoวมเสื้อสีฟ้าสีเหลืองกำลังสีเหลืองและสีแดงวางอยู่บนจ',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'รารารารารารารารารารารารารารารารารารา',\n",
       " 'photoวมเสื้อสีเขียวสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนคนคนกำลังอยู่บนโต๊ะมีคนสีน้ำตาลสีน้ำตาล',\n",
       " 'คนกำลังจอดอยู่บนถนนมีต้นไม้',\n",
       " 'คนกำลังกำลังกำลังสีเขียวกำลังอยู่บนโต๊ะ',\n",
       " 'แมวสีน้ำตาลกำลังกำลังกำลังกำลังกำลังอยู่บนเตียง',\n",
       " 'คนคนคนคนคนคนกำลังกำลังคนกำลังคนกำลังอยู่ในสนามหญ้า',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงพื้น',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนคนคนกำลังอยู่บนถนน',\n",
       " 'ที่กำลังอยู่บนถนน',\n",
       " 'คนคนคนคนคนคนคนกำลังเล่นเล่นกีฬาสีแดงอยู่ตรงสนาม',\n",
       " 'คนคนคนกำลังเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่นเล่น',\n",
       " 'คนกำลังสีน้ำตาลสีแดงกำลังสีน้ำตาลสีแดงอยู่บนถนน',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาลสี',\n",
       " 'คนกำลังจอดอยู่บนถนนมีคนจอดอยู่',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'คนคนคนกำลังกำลังกำลังกำลังอยู่บนถนน',\n",
       " 'ราฟราฟสีเขียวยืนอยู่บนพื้นหญ้าสีเขียว',\n",
       " 'คนคนกำลังกำลังกำลังกำลังอยู่ตรงถนน',\n",
       " 'คนคนคนคนกำลังกำลังกำลังกำลังกำลังกำลังกำลังกำลังอยู่บนโต๊ะ',\n",
       " 'ขนมสีเขียววางอยู่บนโต๊ะสีเขียววางอยู่บนโต๊ะ',\n",
       " 'คนคนกำลังสีน้ำตาลสีน้ำตาลสีน้ำตาลอยู่ในสีน้ำตาลสีน้ำตาลสีน้ำตาลสีน้ำตาล',\n",
       " 'คนกำลังกำลังกำลังกำลังกำลังอยู่ตรงสนามหญ้า',\n",
       " 'photo난สีเหลืองสีเหลืองกำลังสีเหลืองสีแดงวางอยู่บนโต๊ะสีเหลือง',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test2017/000000160477</td>\n",
       "      <td>คนคนคนกำลังกำลังกำลังกำลังอยู่บนโต๊ะ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test2017/000000386306</td>\n",
       "      <td>คนกำลังกำลังอยู่ตรงถนนมีคนกำลังอยู่ตรงถนน</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test2017/000000502273</td>\n",
       "      <td>ที่กำลังสีเหลืองอยู่บนพื้น</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test2017/000000480896</td>\n",
       "      <td>คนคนกำลังอยู่บนโต๊ะสีเขียววางอยู่บนโต๊ะ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test2017/000000228698</td>\n",
       "      <td>คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image_id                                    caption\n",
       "0  test2017/000000160477       คนคนคนกำลังกำลังกำลังกำลังอยู่บนโต๊ะ\n",
       "1  test2017/000000386306  คนกำลังกำลังอยู่ตรงถนนมีคนกำลังอยู่ตรงถนน\n",
       "2  test2017/000000502273                 ที่กำลังสีเหลืองอยู่บนพื้น\n",
       "3  test2017/000000480896    คนคนกำลังอยู่บนโต๊ะสีเขียววางอยู่บนโต๊ะ\n",
       "4  test2017/000000228698       คนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคนคน"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['caption'] = test_prediction\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('/home/hpcnc/cloud/SuperAI/Hack-Image-Caption/resource/gg.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
